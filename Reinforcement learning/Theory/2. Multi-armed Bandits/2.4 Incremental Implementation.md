The [[2.2 Action-value Methods| action-value methods]]: estimate action's value as sample averages of observed rewards.

-> How can we compute these averages computationally efficient?
$\mathbf O(1)$ memory and $\mathbf O(n)$ per-time-step computation.

Let $Q_n$ denote the estimate of its action's value after it has been selected $n-1$ times, we have:
$$Q_n = \frac{1}{n-1}\sum_{i=1}^{n-1}R_i
$$

^a686db

-> $\mathbf O(n)$ memory and $\mathbf O(n)$ time.

Better:

$$
\begin{align}
Q_{n+1} &= \frac 1 n \sum_{i=1}^n R_i \\
& =\frac 1 n \Bigg ( R_n + \sum_{i=1}^{n-1} R_i\Bigg) \\
& = \frac 1 n \Bigg ( R_n + (n-1) \frac 1 {n-1} \sum_{i=1}^ {n-1} R_i \Bigg) \\
& = \frac 1 n \Big ( R_n + (n-1)Q_n \Big ) \\
& = \frac 1 n \Big (R_n +  nQ_n - Q_n\Big) \\
& = Q_n + \frac 1 n [R_n - Q_n]
\end{align}
$$

^88f053

-> $\mathbf O(1)$ memory and $\mathbf O(n)$ time.

General form:

$$
\text{New estimation} \leftarrow \text{Old estimation} + \text{Step Size } [\underbrace {\text{Target} - \text{Old Estimation}}_{\text{Error}}] 
$$
The target is the $n$th reward.

Simple Bandit algorithm:

> [!NOTE] Simple Bandit algorithm:
> **function** bandit(Action a):
> ----**return** Reward
> 
> Initialize : **for** a=1 **to** k:
> ----$Q(a) \leftarrow 0$
> ----$N(a) \leftarrow 0$
> 
> Loop:
> $$
> \begin{align}
> &A \leftarrow \begin{cases} \arg \max_a Q(a) & \text{prob = } 1 - \epsilon \\ \text{a random action} & \text{prob = } \epsilon \end{cases} \\
> &R \leftarrow \text{bandit}(A) \\
> &N(A) \leftarrow N(A) + 1 \\
> &Q(A) \leftarrow Q(A) + \frac 1 {N(A)} [R-Q(A)]
> \end{align}
> $$




