It'd be better to select among the non-greedy actions according to their potential for actually being optimal.

One effective way of doing this is to select actions according to:
$$
A_t = \arg \max_{a} \Bigg [ Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}} \Bigg]
$$
$N_t(a)$ denotes the number of times that action $a$ has been selected prior to time $t$ (denominator in [[2.2 Action-value Methods#^a154a8|this]])

$c>0$ controls the degree of exploration.

If $N_t(a)=0$, then $a$ is considered to be a maximizing action.

The idea of this upper confidence bound (UCB) action selection is that the square-root term is measure of the uncertainty or variance in the estimate of $a$'s value.


