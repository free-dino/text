Consider learning a numerical preference for each [[2.1 A k-armed Bandit Problem#^27ecc8|action]] $a$, which denote $H_t(a) \in \R$. 
-> The larger the preference, the more often that action is taken -> no interpretation in terms of reward.

Soft-max distribution (Gibbs or Boltzmann distribution) (see: [[2. Classification and Logistic Regression#^8ff797|soft-max function]]):
$$
\mathbf P(A_t =a) \doteq \frac {e^{H_t(a)}}{\sum_{b=1}^k e^{H_t(b)}} \doteq \pi_t(a)
$$
$\pi_t(a)$ denotes the probability of taking action $a$ at time $t$.

Initially all action preferences are the same.

There is a natural learning algorithm for soft-max action preferences based on the idea of stochastic gradient ascent. After selecting action $A_t$ and receiving reward $R_t$, the action preferences are updated by:

$$
\begin{align}
H_{t+1}(A_t) &\doteq H_t(A_t) + \a(R_t - \bar R_t)(1-\pi_t(A_t)), & \text{and} \\
H_{t+1}(a) &\doteq H_t(a) - \a(R_t - \bar R_t) \pi_t(a), & \forall a \not = A_t
\end{align}
$$

^f401a8

where $\a > 0$ is a step-size parameter.
$\bar R_t \in \R$ is the average of the rewards up to, but not including time $t$ ($\bar  R_1 \doteq R_1$)


The $\bar R_t$ serves as a baseline with which the reward is compared. If the reward is higher than the baseline, the probability of taking $A_t$ in the future is increased and opposite.


> [!NOTE] The Bandit Gradient Algorithm as Stochastic Gradient Ascent
> In exact gradient ascent, each action preference $H_t(a)$ would be incremented in proportion to the increment's effect on performance:
> $$
> H_{t+1}(a) \doteq H_t(a) + \a \frac {\partial \E[R_t]}{\partial H_t(a)}
> $$ 
> where the measure of performance here is the expected reward:
> $$
> \E[R_t] = \sum_x \pi_t(x)q_*(x)
> $$
> -> It's not possible to implement gradient ascent exactly in our case because by assumption we do not know the $q_*(x)$, but in fact the updates from [[2.8 Gradient Bandit Algorithms#^f401a8|this]] are equal to the formula above in expected value, making the algorithm an instance of stochastic gradient ascent.
> 
> We have:
> $$
> \begin{align}
> \frac {\partial \E[R_t]}{\partial H_t(a)} & = \frac \partial {\partial H_t(a)} \Bigg [ \sum_x \pi_t(x)q_*(x) \Bigg] \\ 
> & = \sum_x q_*(x) \frac {\partial \pi_t(x)} {\partial H_t(a)} \\
> & = \sum_x \big ( q_*(x) - B_t \big ) \frac {\partial \pi_t(x)} {\partial H_t(a)}
> \end{align}
> $$
> 
> $B_t$ called the baseline, can be any scalar that does not depend on $x$.
> 
> $\sum_x \frac {\partial \pi_t(x)} {\partial H_t(a)} = 0$. As $H_t(a)$ changed, some actions' probability go up and some go down, but the sum of changes must be zero because the sum of probability is always one.
> 
> Multiply each term of the sum by $\frac{\pi_t(x)}{\pi_t(x)}$:
> $$
> \frac {\partial \E[R_t]}{\partial H_t(a)} = \sum_x \pi_t(x) \big ( q_*(x) - B_t \big ) \frac {\partial \pi_t(x)}{\partial H_t(a)}
> $$






