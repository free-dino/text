A problem: 
- Facing repeatedly
- Choice among $k$ different options, or actions
- After each choice, we receive a numerical reward chosen from a stationary probability distribution.
- Objective: maximize the expected total reward over some time period, aka time steps.

Each of the $k$ actions has an expected or mean reward given that that action is selected -> value of that action. ^701c22

Denote: 
- The action selected on time step $t$ as $A_t$, and the corresponding reward as $R_t$. ^611402
- The value of an arbitrary action $a$, $q_*(a) = \E[R_t \mid A_t =a]$. ^27ecc8

-> Select the action with highest value.

Assume: You do not know the action's values with certainty, although you may have estimates.

Denote: Estimated value of action $a$ at the time step $t$ as $Q_t(a)$. ^b55b67

-> We want $Q_t(a)$ to be close to $q_*(a)$.

If: maintain estimates of the action values -> then: at any step, there is at least one action whose estimated value is greatest. -> Greedy actions.

When: select one of these actions -> exploiting our current knowledge of the values of the actions.

If we choose one of the non-greedy actions, we say you are exploring -> enables you to improve your estimate of the non -greedy action's value.

Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run.