To roughly assess the relative effectiveness of the greedy and $\epsilon-$greedy [[2.2 Action-value Methods|action-value method]], we compared them numerically on the suite of test problems.

2000 randomly generated [[2.1 A k-armed Bandit Problem|k-armed bandit problems]] with $k=10$.
Each bandit problem, such as: 
![[Pasted image 20241002094328.png]]
The [[2.1 A k-armed Bandit Problem#^27ecc8|action values]] $q_*(a),\  a=1,\dots,10,$ were selected according to a normal distribution with mean $0$ and variance $1$. 

When a learning method applied to that problem selected [[2.1 A k-armed Bandit Problem#^611402|action]] $A_t$ at time step $t$, the [[2.1 A k-armed Bandit Problem#^611402|actual reward]], $R_t$ was selected from a normal distribution with mean $q_*(A_t)$ and variance 1. This suite of test tasks the $10-$armed testbed.

For any learning method, we can measure its performance and behavior as it improves with experience over $1000$ time steps when applied to one of the bandit problem -> this makes up one run -> repeating this for 2000 independent runs, each with a different bandit problem, we obtained measures of the learning algorithm's average behavior.

Compare a [[2.2 Action-value Methods#^f6c5ce|greedy method]] with 2 $\epsilon$-[[2.2 Action-value Methods#^53bbe4|greedy methods]] ($\epsilon=0.01$ and $\epsilon=0.1$) on the testbed -> Action-value estimates using the sample-average technique with initial estimate of 0.
![[Pasted image 20241006192104.png]]

Pros of $\epsilon-$greedy over greedy depends on the task.

Noisier rewards -> $\epsilon-$greedy.
Variance = $0$ -> greedy.

# Exercise: 
2.2

| k   | A   | R   | $Q_n$             | $\epsilon$ |
| --- | --- | --- | ----------------- | ---------- |
| 1   | 1   | -1  | {0, 0, 0, 0}      | $\sim$     |
| 2   | 2   | 1   | {-1, 0, 0, 0}     | $\sim$     |
| 3   | 2   | -2  | {-1, 1, 0, 0}     | $\sim$     |
| 4   | 2   | 2   | {-1, -0.5, 0 , 0} | x          |
| 5   | 3   | 0   | {-1, 1/3, 0, 0}   | x          |
