Estimating the [[2.1 A k-armed Bandit Problem#^b55b67|value of actions]] and using the estimates to make action selection decisions -> action-value methods. 
$$Q_t(a) \overset{.}{=} \frac {\text{sum of rewards when a taken prior to t}}{\text{number of times a taken prior to t}} = \frac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{I}\{A_i=a\}}{\sum_{i=1}^{t-1} \cdot \mathbb I\{A_i=a\}}$$ ^a154a8

If the denominator $=0$, $Q_t(a)$ will be assigned to some default value, such as 0.

As the denominator goes to infinity, by the law of [[5.2 The weak law of large numbers|large numbers]], $Q_t(a)$ converges to $q_*(a)$. -> sample average method for estimating action values -> this is just one way to estimate action values, and not necessarily the best one.

Greedy action selection method: $$A_t = \arg \max_{a}Q_t(a)$$ where $\arg\max_a$ denotes the action $a$ for which expression that follows is maximized. ^f6c5ce

Greedy action selection always exploits current knowledge to maximize the immediate reward.

Alternative: behave greedy most of the time but every once in a while say with a small probability $\epsilon$, instead select randomly from among all the actions with equal probability, independently of the action-value estimates. -> $\epsilon-$greedy methods. ^53bbe4

Pros:
- In the limit as the number of steps increases, every action will be sampled an infinite number of times, thus ensuring that all the $Q_t(a)$ converge to their respective $q_*(a)$. $\implies$ the probability of selecting the optimal action converges to greater than $1-\epsilon$ to near certainty-> just asymptotic guarantees and say little about the practical effectiveness of the methods.