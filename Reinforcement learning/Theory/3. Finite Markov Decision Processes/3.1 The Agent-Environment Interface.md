- The learner and decision maker is called the *agent*.
- The thing it interact with, comprising everything outside the agent, is called *environment*
- ![[Pasted image 20241106160428.png]]
- At time step $t$, the agent receives some representation of the environment's *state*, $S_t \in \mathcal S$, and on that basis selects an *action*, $A_t \in \mathcal A(s)$.
- One time step later, the agent receives a numerical *reward*, $R_{t+1} \in \mathcal R \subset \R$ and finds itself in a new state $S_{t+1}$.
- The MDP and agent give rise to a trajectory that begins like $$S_0, A_0, R_1, S_1, A_1, R_2, S_2, A_2, R_3,\dots$$
- $\mathcal S, \mathcal A, \mathcal R$ are finite.
- For particular values of the of these random variables, $s' \in \mathcal S$ and $r \in \mathcal R$, there is a probability of those values occurring at time $t$: $$p(s', r \mid s, a) \doteq Pr\{S_t = s', R_t = r \mid S_{t-1}=s, A_{t-1} = a\} $$, for all $s, s' \in \mathcal S, r\in \mathcal R$ and $a \in \mathcal R(s)$. 
- The function $p$ defines the dynamics of the MPD. $$p: \mathcal S \times \mathcal R \times \mathcal S \times \mathcal A \to [0, 1]$$, is an ordinary deterministic function of four arguments. and must satisfy: $$\sum_{s' \in \mathcal S} \sum_{r \in \mathcal R} p(s', r \mid s, a) = 1, \qquad \forall s \in \mathcal S, a \in \mathcal A(s)$$