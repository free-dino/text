Decomposition of $\bar E_{new}$ into a squared bias and variance term as well as an unavoidable component of irreducible noise.

The general concepts of bias and variance:
- Consider an experiment with an unknown constant $z_0$ that we want to estimate
- To estimate $z_0$ we have a random variable $z$.
- Because $z$ is a random variable, it have some mean $\mathbb E[z]$, denoted by $\bar z$
Now we can define:
$$
\begin{align}
\text{Bias } &: \mathbb E_z[\bar z - z_0] = \bar z - z_0 \\
\text{Variance } &: \mathbb E_z[(z-\bar z)^2] = \mathbb E[z^2] - \bar z^2
\end{align}
$$

The variance describes how much the experiment varies each time we perform it.
The bias describes the systematic error in $z$ that always remain

Consider the expected squared error between $z$ and $z_0$ as a metric of how good the estimator $z$ is, in terms of variance and the squared bias:
$$
\begin{align}
\mathbb E_z[(z-z_0)^2] &= \mathbb E_z[\big( (z- \bar z) + (\bar z - z_0) \big)^2] \\
& =\mathbb E_z [(z-\bar z)^2 + (\bar z - z_0)^2 + 2(z-\bar z)(\bar z - z_0)] \\
& = \mathbb E_z[(z-\bar z)^2] + 2 \mathbb E_z[(z-\bar z)] \mathbb E_z[(\bar z - z_0)] + \mathbb E_z[(\bar z -z_0)^2] \\
& = \E_z[(z-\bar z)^2] + 2\E_z[(z-\bar z)] (\bar z - z_0) + (\bar z - z_0)^2 \\
& = \E_z[(z-\bar z)^2] + 2(\E_z[z] - \bar z) (\bar z - z_0) + (\bar z - z_0)^2 \\
& = \E_z[(z-\bar z)^2] + 2(\bar z - \bar z) (\bar z - z_0) + (\bar z - z_0)^2 \\
& = \underbrace{\E_z[(z-\bar z)^2]}_\text{Variance} + \underbrace{(\bar z - z_0)^2}_\text{bias squared}
\end{align}
$$
The main point here is that to obtain a small expected squared error, we have to consider both the bias and the variance. Both aspects are important

We make the assumption that the true relationship between input $\v x$ and out put $y$ can be described as some function: 
$$y = f_0(\v x) + \epsilon$$ with $\E [\epsilon] = 0$ and $\text{Var}[\epsilon] = \sigma^2$ 

In our notation, $\hat y(\v x, \mathcal T)$ represents the model when it's trained on training data $\mathcal T$. This is our random variable $z$

The (hypothetical) average trained model we would achieve, corresponding to $\bar z$:
$$
\bar f(\v x) \overset{\D}{=} \E_{\mathcal T}[\hat y(\v x; \mathcal T)]
$$
Remember the [[3. The Training Error-Generalization Gap Decomposition of E_new#^468435|definition]] of $\bar E_{new}$ for regression with squared error is:
$$
\bar E_{new} = \E_{\mathcal T} \big [ \E_* [(\hat y(\v x_*; \mathcal T)-y_*)^2]\big]
$$
We can rewrite it as:
$$
\begin{align}
\bar E_{new} &= \E_{\mathcal T} \Big [ \E_{*} [(\hat y(\v x_*; \mathcal T)-f_0(\v x_*)-\epsilon_*)^2]\Big] \\
& = \E_{\mathcal T} \Big [ \E_{*} [\underbrace{(\hat y(\v x_*; \mathcal T)}_{z}-\underbrace{\bar f(\v x_*)}_{\bar z} + \underbrace{\bar f(\v x_*)}_{\bar z} - \underbrace{f_0(\v x_*)}_{z_0}-\epsilon_*)^2]\Big] \\
& = \E_{\mathcal T} \Big [ \E_{z*} [\big((z-\bar z) + (\bar z - z_0)-\epsilon_* \big)^2]\Big] \\
& = \E_{\mathcal T} \Big [ \E_{z*}[(z-\bar z)^2] + \E_{z*}[(\bar z - z_0)^2] + \E_{z*} [\epsilon_*^2] \\ & \qquad+ 2\E_{z*}[(z-\bar z)(\bar z - z_0)] - 2 \E_{z*}[(z-\bar z)\epsilon] -2 \E_{z*}[(\bar z - z_0) \epsilon]\Big] \\ 
\end{align}
$$
With $\E[\epsilon] = 0$ and $\text{Var}[\epsilon] = \E[\epsilon^2] - \E[\epsilon]^2 = \sigma^2 \iff \E[\epsilon^2] = \sigma^2$, so we can reduce the equation above into:
$$ 
\begin{align}
 \bar E_{new} &= \E_{\mathcal T} \Big [ \E_{z*}[(z-\bar z)^2] + (\bar z - z_0)^2 + \epsilon_*^2 \Big] \\
 &= \E_{\mathcal T} [\E_{z*}[(z-\bar z)^2] + \E_{z*}[(\bar z - z_0)^2] + \sigma_*^2] \\
 & = \E_{\mathcal T}[\E_{*}[(\hat y(\v x; \mathcal T) - \bar f(\v x_*))^2]] + \E_{\mathcal T}[\E_{*}[(\bar f(\v x_*)- f_0(\v x_*))^2]] + \E_{\mathcal T}[\sigma_*^2] \\
 & = \underbrace{\E_{*}[\E_{\mathcal T}[(\hat y(\v x; \mathcal T) - \bar f(\v x_*))^2]]}_{\text{Variance}} + \underbrace{\E_{*}[(\bar f(\v x_*)- f_0(\v x_*))^2]}_{\text{squared bias}} + \underbrace{\sigma_*^2}_\text{irreducible error} 
 \end{align}
$$

The squared bias now describe how much the average trained model $\bar f(\v x_*)$ differs from the true $f_0(\v x_*)$, averaged over all possible test data

The variance term describe how much $\hat y(\v x; \mathcal T)$ varies each time the model is trained on a different training dataset.

## What Affects the Bias and Variance?

A high model complexity means low bias and high variance
A low model complexity mean high bias and low variance

The more flexible the model is, the more it will adapt the training data $\mathcal T$, even to noise
Low flexibility can be too rigid to capture the true relationship between inputs and outputs

The more data, the more information we have about the parameters, resulting in a smaller variance
## Connection Between Bias, Variance and the [[3. The Training Error-Generalization Gap Decomposition of E_new#^b73775|Generalization Gap]]
In practice, we mostly have an estimate of the generalization gap (for example as $E_{hold-out} - E_{train}$), 

Consider the regression problem. Assume that the squared loss function is used both as [[1. Expected New Data Error E_new - Performance in Production#^3b6a55|error function]] and [[1. Linear regression#^852d2e|loss function]] and that global minimum is found during training. We can then write:
$$
\begin{align}
\sigma^2 + \text{bias}^2 &= \E_*[\epsilon_*^2] + \E_*[(\bar f(\v x_*) - f_0(\v x_*))^2] \\ 
&= \E_*[(\epsilon_* + f_0(\v x_*) - \bar f(\v x_*))^2 - 2\epsilon_* (\bar f(\v x_*) - f_0(\v x_*))]\\
& = \E_* [(\bar f(\v x_*) - y_*)^2] - E_*[2\epsilon_* (\bar f(\v x_*) - f_0(\v x_*))] \\
&=\E_*[(\bar f(\v x_*) - y_*)^2]
\end{align}
$$
and 
$$
\E_*[(\bar f(\v x_*) - y_*)^2] \approx \frac 1 n \sum_{i=1}^n (\bar f(\v x_i) - y_i)^2 \ge \frac 1 n \sum_{i=1}^n (\hat y(\v x_i; \mathcal T)-y_i)^2 = E_{train}
$$
because $\hat y(\v x; \mathcal T)$ always finding the global minimum. 
Remembering that $\bar E_{new} = \sigma^2 + \text{bias}^2 + \text{variance}$ and allowing ourselves to write $\bar E_{new} - E_{train} = \text{generalization gap}$, we have:
$$
\begin{align}
\text{generalization gap} & \gtrapprox \text{variance}, \\
E_{train}& \lessapprox \text{bias}^2 + \sigma^2
\end{align}
$$

The assumptions in this derivation are not always met in practice, but at least gives us some rough idea.

[[3. The Training Error-Generalization Gap Decomposition of E_new#^71692e|The choice of method is crucial]] for what $E_{new}$ is obtained
Decreased bias does not always lead to increase variance; and vice versa.

In contrast to the [[3. The Training Error-Generalization Gap Decomposition of E_new|decomposition of E_new into training error and generalization gap]], the bias and variance decomposition can shed some more light on why $E_{new}$ is decreased for different methods: sometimes, the superiority of one method over another can be attributed to either a lower bias or a lower variance.
