## The confusion Matrix and the ROC Curve
If we learn a binary classifier and evaluate it on a hold-out validation dataset. We can use a confusion matrix to inspect the performance beyond just computing $E_{hold-out}$

|                   | $y=-1$ | $y=1$ | total |
| ----------------- | ------ | ----- | ----- |
| $\hat y(\v x)=-1$ | TN     | FN    | N*    |
| $\hat y(\v x)=1$  | FP     | TP    | P*    |
| **total**         | N      | P     | n     |
For asymmetric problems, it's important to distinguish between FP(type I error) and FN(type II error)

The trade-off between those 2 errors can often be done by tuning a decision threshold $r$

Some common terms related to the quantities in the confusion matrix:

| Name                                                          | Formular                                       |
| ------------------------------------------------------------- | ---------------------------------------------- |
| FP rate, Fall-out, Probability of false alarm                 | $\frac {FP} N$                                 |
| TN rate, Specificity, Selectivity                             | $\frac{TN} N$                                  |
| TP rate, Sensitivity, Power, Recall, Probability of detection | $\frac {TP} P$                                 |
| FN rate, Miss rate                                            | $\frac {FN} P$                                 |
| Positive predictive value, Precision                          | $\frac {TP}{P*}$                               |
| False discovery rate                                          | $\frac{FP}{N*}$                                |
| Negative predictive value                                     | $\frac{TN}{N*}$                                |
| False omission rate                                           | $\frac{FN}{N*}$                                |
| Prevalence                                                    | $\frac P n$                                    |
| Misclassification rate                                        | $\frac{(FN + FP)}{n}$                          |
| Accuracy, 1-misclassification rate                            | $\frac{(TN + TP)}{n}$                          |
| $F_1$ score                                                   | $\frac{2TP}{(P* +P)}$                          |
| $F_\b$ score                                                  | $\frac{(1+\b^2)TP}{(1+\b^2)TP + \b^2 FN + FP}$ |
- Recall describe how large a proportion of the positive data points are correctly predicted as positive. A high recall is good, and a low recall indicates a problem with many false negatives
- Precision describes what the ratio of true positive points are among the ones predicted as positive. A high precision is good and a low precision indicates a problem with many false positive.
- If we want to compare different classifiers for a certain problem without specifying a certain decision threshold $r$, the ROC curve can be useful.

TO plot an ROC curve, the recall/true positive (TP/P: large value is good) rate is drawn against the false positive rate (FP/N: small value is good) for all values of $r \in [0,1]$. A compact summary of the ROC curve is the area under the ROC curve, ROC-AUC -> closer to 1 is good.

## The $F_1$ score and the precision-recall curve
Many binary classification problems have particular characteristics. They are either:
- imbalanced if the vast majority of the data points belong to one class, typically the negative class $y = -1$
- asymmetric if false negative is considered more severe than false positive or vice versa