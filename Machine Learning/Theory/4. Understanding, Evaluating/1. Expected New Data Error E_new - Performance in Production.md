An error function $E(\hat y, y)$ encoding the purpose of classification or regression. 

A loss function is used when learning/ training 
An error function is used to analyze performance

$E$ return a small value if the prediction is good and a larger value otherwise

Misclassification: $E(\hat y, y) \overset{\Delta}{=} \mathbb I \{\hat y \not = y\} = \begin{cases} 0 & \text{if } \hat y = y \\ 1 & \text{if } \hat y \not = y \end{cases}$ 
Squared error: $E(\hat y, y) \overset{\Delta}{=} (\hat y -  y)^2$  ^3b6a55

The problem with misclassification is that it's not continuous $\implies$ it not suitable for use as an optimization objective

We use $p(\v x, y)$ to describe the endless stream of new data, which is extremely complicated and impossible to write out. We will just use $p(\v p; y)$ to reason about supervised learning methods

We use a function $f(\v x, \vbs \theta)$ to describe $\mathcal T = \{\v x_i, y_i\}_{i=1}^n$. Our job is to learn $\hvbs \theta \implies f(\v x ,\hvbs \theta) = \v {\hat y}_*$  

With $(\v x_*; y_*)$ as a random variable represent all the possible test data points with respect to the distribution $p(\v x, y)$, this is referred as expected new data error,
$$
E_{new} \overset {\Delta}{=} \mathbb E_*[E(\hat y (\v x_*; \mathcal T), y_*)] = \mathbb E_*[E(\hat y_*, y_*)]
$$

^280ed5

where the expectation $\mathbb E_*$ is the expectation over all possible test data points with respect to the distribution $(\v x_*, y_*) \sim p(\v x, y)$, that is,
$$
\mathbb E_*[E(\hat y (\v x_*; \mathcal T), y_*)] = \int E(\hat y (\v x_*; \mathcal T), y_*) p(\v x_*, y_*) d\v x_*dy_*
$$
$E_{new}$ describe how well the model generalize the problem

Training error:
$$
E_{train} \overset{\Delta}{=} \frac 1 n \sum _{i=1}^n E(\hat y (\v x_i; \mathcal T), y_i)
$$

^9be216

This simply describes how well a method performs on the specific data on which it was trained.

For $k$-NN with $k=1$, $E_{train}=0$ and $E_{new} \not =0$

Our goal is to achieve as small $E_{new}$ as possible
