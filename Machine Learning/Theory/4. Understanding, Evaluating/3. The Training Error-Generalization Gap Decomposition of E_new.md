We use $E_{new}(\mathcal T)$ and $E_{train}(\mathcal T)$ (with the definition of [[1. Expected New Data Error E_new - Performance in Production#^280ed5|E_new]] and [[1. Expected New Data Error E_new - Performance in Production#^9be216|E_train]]) to emphasize the fact that they both are conditional on a specific training dataset $\mathcal T$. Let's now introduce
$$
\bar{E}_{new} \overset{\Delta}{=} \mathbb E_{\mathcal T}[E_{new} (\mathcal T)]
$$

^468435

and 
$$
\bar{E}_{train} \overset{\Delta}{=} \mathbb E_{\mathcal T}[E_{train}(\mathcal T)]
$$
It usually holds that
$$
\bar{E}_{train} < \bar E_{new}
$$
and 
$$
\text{generalization gap} \overset{\Delta}{=} \bar{E}_{new} - \bar{E}_{train}
$$

^b73775

With the decomposition of $\bar{E}_{new}$ into
$$
\bar{E}_{new} \overset{\Delta}{=} \bar{E}_{train} + \text{generalization gap}
$$
## What effect the generalization gap?
The generalization gap depends on the method and the problem

The more a methods adapt to training data, the larger the generalization gap.

A framework for how much a methods adapts to training data is given by the Vapnik-Chervonenkis (VC) dimension. ^dc6deb

Typically: higher model complex implies a larger generalization gap
![[Pasted image 20240803205710.png]]

The more training data, the smaller the generalization gap.

## Reducing $E_{new}$ in practice
Let's draw two conclusions from what we have seen so far:
- $E_{new}$ will, on average, not be smaller than $E_{train}$. Thus, if $E_{train}$ is much bigger than $E_{new}$, you should re-think the problem.
- The generalization gap and $E_{new}$ typically decrease as $n$ increase. Thus, if possible, increasing the size of the training data may help a lot with reducing $E_{new}$.

By monitoring $E_{train}$ and estimating $E_{new}$ with [[2. Estimating E_new#^c2d70c|cross-validation]], we also get the following advice
- If $E_{hold-out} \approx E_{train}$ (small generalization gap; possibly underfitting), it might be beneficial to increase the model flexibility by loosening the regularization increase the model order (more parameters to learn), etc
- If $E_{train}$ is close to 0 and $E_{hold-out}$ is not (possibly overfitting), it might be beneficial to decrease the model flexibility by tightening the regularization, decrease the order (fewer parameters to learn), etc

## Shortcomings of the Model Complexity Scale
For a given problem, one method can have a smaller generalization gap than another method without having a larger training error

Some methods are simply better for certain problem ^71692e