Continue with the [[1. Expected New Data Error E_new - Performance in Production|previous section]],
## $E_{train} \not \approx E_{new}$: We cannot Estimate $E_{new}$ from training data
> [!NOTE] Important
> $E_{new}$ is incomputable, we can only find a way to estimate it.
> $E_{train}$ is computable but give us no information about real world data 

The performance on training data $E_{train}$ is therefore not a reliable estimate of $E_{new}$
Typically $E_{train} < E_{new}$ (not always the case tho)
## $E_{hold-out} \approx E_{new}$: We can Estimate $E_{new}$ from Hold-Out Validation Data
We cannot use out training data to compute $E_{new}$ directly since this would means that we evaluate the model on the data it has seen before

Set aside some hold-out validation data $\{\v x'_j, y'_j\}_{j=1}^{n_v}$  from $\mathcal T$
$$
E_{hold-out} \overset{\Delta}{=} \frac 1 {n_v} \sum_{j=1}^{n_v} E(\hat y(\v x'_j; \mathcal T), y'_j)
$$
![[Pasted image 20240803160909.png]]

> [!Aware] Be aware!
> Shuffling the data before perform the train-val split.

$$
\text{Var}[E_{hold-out}] \downarrow \iff n_v \uparrow
$$
A small the variance of $E_{hold-out}$ means that we can expect it to be close to $E_{new}$

> [!NOTE] Alert
> In a data-rich situations, the hold-out validation data approach is sufficient


## $k$-Folds Cross-Validation: $E_{k-fold} \approx E_{new}$ Without Setting Aside Validation Data

^c2d70c

![[Pasted image 20240803162022.png]]
5 steps procedure:
1. Split the dataset into $k$ batches of similar size, and let $\ell=1$
2. take batch $\ell$ as the hold-out validation data and the remaining batches as training data
3. train the model on the training data, and compute $E^{(\ell)}_{hold-out}$ as the average error on the hold-out validation data ^15fea5
4. if $\ell < k$, then $\ell \leftarrow \ell + 1$ and return to [[#^c0ac03|step 2]]. If $\ell = k$ compute the $k$-fold cross-validation error 
   $$
   E_{k-fold} \overset{\Delta}{=} \frac 1 k \sum_{\ell =1}^k E^{(\ell)}_{hold-out}
   $$
5. train the model again, this time using the entire dataset ^53dc5d

We can also split the dataset into $n$ batches with size = 1 $\implies$ LOO cross validation when the accuracy is really important

Now, we need to understand why $k$-fold cross-validation works:
- We have to distinguish between final model ([[#^53dc5d|step 5]]) and the intermediate models are trained on $\frac {(k-1)} k$ fraction of the data in [[#^15fea5|step 3]] 
- Each intermediate $E_{hold-out}^{(\ell)}$ is an unbiased but high-variance estimate of $E_{new}$ for the corresponding $\ell th$ intermediate model.

We can use [[3. The Training Error-Generalization Gap Decomposition of E_new#^dc6deb|Grid Search CV]] to find the best model
## Using a test dataset
If we selecting hyperparameters based on $E_{k-fold}$ (or $E_{hold-out}$), there is a risk of overfitting to the validation data 
Using the test set only once