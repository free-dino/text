## Stochastic gradient descent
With very large $n$, we can expect the gradient computed only for the first half of the dataset $\nabla_{\vbs \theta} J(\vbs \theta) \approx \sum_{i=1}^{n/2}\nabla_{\vbs \theta}L(\v x_i, \v y_i, \vbs \theta)$ to be almost identical to the gradient based on the second half of the dataset $\nabla_{\vbs \theta} J(\vbs \theta) \approx \sum_{i=n/2+1}^{n}\nabla_{\vbs \theta}L(\v x_i, \v y_i, \vbs \theta)$
update:
$$
\begin{align}
\vbs\theta^{(t+1)} &= \vbs \theta{(t)} - \gamma \frac {1} {n/2} \sum_{i=1}^{n/2} \nabla_{\vbs \theta}L(\v x_i, \v y_i, \vbs\theta^{(t)}) \\
\vbs\theta^{(t+2)} &= \vbs \theta{(t+1)} - \gamma \frac {1} {n/2} \sum_{i=\frac n2+1}^{n} \nabla_{\vbs \theta}L(\v x_i, \v y_i, \vbs\theta^{(t +1 )}) 
\end{align}
$$
-> extend from 2 batch -> mini-batch -> 1 complete pass -> 1 epoch

---
Algorithm: Stochastic gradient descent

---
Input: Objective function $J(\vbs \theta) = \frac 1 n \sum_{i=1}^n L(\v x_i, y_i, \vbs \theta)$, initial $\vbs \theta^{(0)}$, learning rate $\gamma^{(t)}$
Result: $\hvbs \theta$
1. Set $t \leftarrow 0$
2. while: convergence criteria not met, do
	1. for $i = 1,2, \dots, E$ do
		1. Randomly shuffle the training data $\{\v x_i, y_i\}^n_{i=1}$
		2. for $j=1,2,\dots, \frac n {n_b}$ do
			1. Approximate the gradient using the mini-batch $$ \{(\v x_i, \v y_i)\}^{jn_b}_{i=(j-1)n_b+1}, \qquad \mathbf{\v {\hat{d}}}^{(t)}= \frac 1{n_b} \sum_{i=(j-1)n_b+1}^{jn_b} \nabla_{\vbs \theta}L(\v x_i, y_1,\vbs \theta^{(t)})$$
			2. Update: $\vbs \theta^{(t+1)} \leftarrow \vbs \theta^{(t)} - \gamma^{(t)}\mathbf{\v {\hat{d}}}^{(t)}$
			3. Update $t \leftarrow t+1$
		End
	End
End
Return $\hvbs \theta \leftarrow \vbs \theta^{(t-1)}$

Extension: momentum, AdaGrad (adaptive gradient), RMSProp (Root mean square propagation), Adam (adaptive moments)

For regression in the 'big data' setting, the stochastic average gradient (SAG) method - averages over all previous estimates 

## Learning Rate and Convergence for Stochastic Gradient Descent
For the algorithm to work, we need the gradient estimate to be unbiased

Gradually decrease the learning rate, the parameter updates will be smaller and smaller -> converge

Hence we start at $t=0$ with a fairly high learning rate $\gamma^{(t)}$ and then decay $\gamma^{(t)}$ as $t$ increase.

Under certain regularity conditions of the cost function and with a learning rate fulfilling the Robbins-Monro conditions $\sum_{t=0}^\infty \gamma^{(t)} = \infty$ and $\sum_{t=0}^\infty(\gamma^{(t)})^2 < \infty$, the stochastic gradient descent algorithm can be shown to almost surely converge to a local minimum. 

In practice, a heuristic for setting the learning rate is:
$$
\gamma^{(t)} = \gamma_\min + (\gamma_\max - \gamma_\min)e^{\frac {-t}{\tau}}
$$
## Adaptive Methods

