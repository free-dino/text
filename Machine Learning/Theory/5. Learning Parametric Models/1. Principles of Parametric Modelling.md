## Non-linear Parametric Functions
Consider the regression model:
$$
y = f_{\vbs \theta} (\v x) + \epsilon
$$
The equation above should be view as our model of true input - output relationship.
It's perhaps most obvious to allow the function $f_{\vbs \theta}$ to be some arbitrary non-linear function.
Training model amounts to finding a suitable value for the parameter vector $\vbs \theta$ such that the function $f_{\vbs \theta}$ accurately describes the true input-output relationship. In mathematical terms, we say that we have a parametric family of functions:
$$
\{f_{\vbs \theta} (\cdot) \mid \vbs \theta \in \mathbf \Theta\}
$$
where $\mathbb \Theta$ is the space containing all possible parameters vectors
The model is more of a "black box" which adapted to fit the training data as well as possible.
For simplicity, we will assume that $\mathbf \Theta = \R^d$, meaning that $\vbs \theta$ is a $d$-dimensional vector of real-valued parameters

The classification models can be constructed in a very similar way, as a way to generalization of the logistic regression model
In [[2. Classification and Logistic Regression#^d8b6dd|binary logistic regression]], we first compute the [[2. Classification and Logistic Regression#^762c14|logit]] $z= \vbs \theta ^ \top \v x$. The probability of the positive class, that is $p=(y=1\mid \v x)$ is then obtained by mapping the logit value through the logistic function, $h(z) = \frac{e^z}{1+e^z}$ 
To turn it into a non-linear classification model, we can simply replace the expression for the logit with $z= f_{\vbs \theta}(\v x)$ for some arbitrary real-valued non-linear function $f_{\vbs \theta}$, Hence the non linear logistic regression model becomes:
$$
g(\v x) = \frac{e^{f_{\vbs \theta}(\v x)}} {1+e^{f_{\vbs \theta}(\v x)}}
$$
The same for multiple class non-linear classifiers
## Loss Minimization as a Proxy for Generalization
For parametric models, the learning objective is typically formulated as an [[2. Optimization Basics|optimization problem]], such as:
$$
\hvbs \theta = \arg \min_{\vbs \theta} \underbrace{\frac 1 n \sum_{i=1}^n \overbrace{L(\pred y {\v x_i, \vbs \theta}, y_i)}^\text{loss function}}_{\text{cost function }J(\vbs \theta)}. 
$$

^03cf34

Most of the time we need to using numerical optimization. This lead to computational trade-off. The longer we run the algorithm, the better solution we expected to find, but at the cost of a longer training time.

But our ultimate goal is to:
$$
\vbs \theta = \arg \min_{\vbs \theta} E_{new}(\vbs \theta)
$$

^2d1e52

where $E_{new}(\vbs \theta) = \E_*[E(\hat y(\v x_*; \vbs \theta), y_*)]$.

> [!NOTE] Important
> The [[1. Principles of Parametric Modelling#^03cf34|training objective]] is only a proxy for the actual [[1. Principles of Parametric Modelling#^2d1e52|objective of interest]]

We make the following observation:
- #### Optimization accuracy vs. statistical accuracy:
  It's not meaning full to optimize $J(\vbs \theta)$ with greater accuracy than the statistical error in the estimate (although in practice, it can be difficult to determine what the statistical accuracy is)
- #### Loss function $\not =$ error function: 
  [[@ List of Understanding, Evaluating, and Improving Performance|Chapter 4]] 
- #### Early stopping:
  We use [[2. Optimization Basics|iterative numerical optimization method]] -> generating a sequence of candidate models -> we have access to the current estimate of the parameter vector -> we obtain a 'path' of parameters values.
  By early stopping, we can obtain a final model with superior performance -> implicit regularization
- ### Explicit regularization
  Adding a term independent of the training data to make the final model generalize better.