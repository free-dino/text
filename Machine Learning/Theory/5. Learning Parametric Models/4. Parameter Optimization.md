## Optimization Using Closed-Form Expressions

We can use coordinate descent to find the optimum of [[3. Regularization#^7581f8|LASSO]]:
$$
\arg \min_{\theta_j} \frac 1 n \norm{\mathbf X \vbs \theta - \v y}_2^2 + \ld \norm{\vbs \theta}_1 = \text{sign}(t)(|t| -\ld)
$$
where: 
$$
t = \sum_{i=1}^n x_{ij}\Big(y_i - \sum_{k \not = j} x_{ik} \theta_k\Big)
$$
## [[4. Gradient Descent|Gradient Descent]] 
Consider the parameter learning problem:
$$
\hvbs \theta = \arg \min_{\vbs \theta} J(\vbs \theta)
$$
Assume the gradient of the cost function $\nabla_{\vbs \theta}J(\vbs \theta)$ exist for all $\vbs \theta$. 
For example, the gradient of the cost function for [[2. Classification and Logistic Regression#^0d4ab9|logistic regression]]:
$$
\nabla_{\vbs \theta} J(\vbs \theta) = -\frac 1 n \sum_{i=1}^n \Big (
\frac {1}{1+e^{y_i \vbs \theta ^\top \v x_i}}\Big)y_i\v x_i,
$$
$\nabla_{\vbs \theta}J(\vbs \theta)$ is a vector of the same dimensions as $\vbs \theta$, which describe the direction in which $J(\vbs \theta)$ increases. -> $-\nabla_{\vbs \theta}J(\vbs \theta)$ describe the direction in which $J(\vbs \theta)$ decreases. That means:
$$
J(\vbs \theta - \gamma \nabla_{\vbs \theta}J(\vbs \theta)) \le J(\vbs \theta)
$$
So that means we should update $\vbs \theta^{(t+1)} = \vbs \theta ^ {(t)} - \gamma \nabla_{\vbs \theta} J(\vbs \theta^{(t)})$

---
**Algorithm: Gradient descent**

---
Input : Objective function $J(\vbs \theta)$, initial $\theta^{(0)}$, learning rate $\gamma$
Result: $\hvbs \theta$
1. Set $t \leftarrow 0$
2. while $\norm{\vbs \theta^{(t)} - \vbs \theta ^ {(t-1)}}$ not small enough, do:
	1. Update $\vbs \theta^{(t+1)} \leftarrow \vbs \theta ^ {(t)} - \gamma \nabla_{\vbs \theta} J(\vbs \theta^{(t)})$
	2. Update $t \leftarrow t+1$
3. end
4. return $\hvbs \theta \leftarrow \vbs \theta^{(t-1)}$


It's possible to formulate the selection of $\gamma$ as an internal optimization problem that is solved at each iteration, a so-called line search problem

No convergence guarantees. 

## Second Order Gradient Methods
Second order Taylor expansion to approximating $J(\vbs \theta)$ around $\vbs \theta^{(t)}$ :
$$
J(\vbs \theta + \v v) \approx \underbrace{J(\vbs \theta) + \v v ^\top [\nabla_{\vbs \theta}J(\vbs \theta)] +\frac 1 2 \v v^\top [\nabla_{\vbs \theta}^2 J(\vbs \theta)]\v v}_{\overset{\Delta}= s(\vbs \theta, \v v)}
$$
where $\v v$ is a vector of the same dimension as $\vbs \theta$.
-> Minimizing $J(\vbs \theta)$ -> Minimizing $s(\vbs \theta, \v v)$. 
If the Hessian $\nabla_{\vbs \theta}^2 J(\vbs \theta)$ is positive definite, then the minimum of $s(\vbs \theta, \v v)$ with respect to $\v v$ is obtained where the derivative of $s(\vbs \theta, \v v)$ is zero:
$$
\frac{\partial}{\partial \v v}s(\vbs \theta, \v v) = \nabla_{\vbs \theta}J(\vbs \theta) + [\nabla_{\vbs \theta}^2 J(\vbs \theta)]\v v = 0 \iff \v v = -[\nabla_{\vbs \theta}^2 J(\vbs \theta)]^{-1} [\nabla_{\vbs \theta} J(\vbs \theta)]
$$
This suggests to update:
$$\vbs \theta^{(t+1)} = \vbs \theta^{(t)} -[\nabla_{\vbs \theta}^2 J(\vbs \theta^{(t)})]^{-1} [\nabla_{\vbs \theta} J(\vbs \theta^{(t)})]$$
-> Newton's method for minimization. 
No convergence guarantees.

Update for trust region only within a ball of radius $D$ around $\vbs \theta^{(t)}$
$$
\vbs \theta^{(t+1)} = \vbs \theta^{(t)} - \eta[\nabla_{\vbs \theta}^2 J(\vbs \theta^{(t)})]^{-1} [\nabla_{\vbs \theta} J(\vbs \theta^{(t)})]
$$
where $\eta \le 1$ is chose as large as possible such that 
$$
\norm{\vbs \theta^{(t+1)} - \vbs \theta^{(t)}} \le D
$$

---
Algorithm: Trust region Newton's method

---

Input: Objective function $J(\vbs \theta)$, initial $\vbs \theta^{(0)}$, trust region radius $D$
Result: $\vbs \theta$
1. Set $t \leftarrow 0$
2. while $\norm{\vbs \theta^{(t)} - \vbs \theta^{(t-1)}}$ not small enough, do:
	1. Compute $\v v \leftarrow [\nabla_{\vbs \theta}^2 J(\vbs \theta^{(t)})]^{-1} [\nabla_{\vbs \theta} J(\vbs \theta^{(t)})]$
	2. Compute $\eta \leftarrow \frac{D}{\max (\normvec v, D)}$ 
	3. Update $\vbs \theta^{(t+1)} \leftarrow \vbs \theta^{(t)} - \eta \v v$
	4. Update $t \leftarrow t+1$
3. end
4. return $\hvbs \theta \leftarrow \vbs \theta^{(t-1)}$

It can be computationally expensive or even impossible to compute the inverse of the Hessian matrix $[\nabla_{\vbs \theta}^2 J(\vbs \theta^{(t)})]^{-1}$ 
To this end, there is an entire class of methods called quasi-Newton methods: the Broyden method, BFGS method (Broyden, Fletcher, Goldfarb and Shano), $\dots$

Limited-memory BFGS or L-BFGS has proven to be another good choice for the [[2. Classification and Logistic Regression|logistic regression problem]]
