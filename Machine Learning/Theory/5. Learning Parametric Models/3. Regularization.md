## $L^2$ regularization
aka Tikhonov regularization, ridge regression and weight decay

It amounts to adding an extra penalty term $\normvec{\boldsymbol \theta }_2^2$ to the cost function.
[[1. Linear regression|Linear regression]] with [[2. Loss Functions and Likelihood-Based Models#^a2940f|squared error loss]] and $L^2$ regularization amounts to solving:
$$
\hvbs \theta = \arg \min_{\vbs \theta} \frac 1 n \norm{\mathbf X \vbs \theta - \v y}_2^2 + \ld \norm{\vbs \theta}_2^2
$$
With $\ld \ge 0$, a trade-off between the original cost function and the regularization term is mad
$\ld =0$ : we recover the original least squares problem
$\ld \rightarrow \infty$ will force all parameters $\hvbs \theta$ to $0$
A good choice of $\ld$ can be determined using [[2. Estimating E_new#^c2d70c|cross-validation]]

$$
\hvbs \theta = (\mathbf X^\top \mathbf X + n\ld \mathbf I_{p+1})^{-1}\mathbf X ^ \top \v y \qquad \text{if } \ld >0
$$

## $L^1$ regularization
aka LASSO, an abbreviation for Least Absolute Shrinkage and Selection Operator
$$
\hvbs \theta =\arg \min_{\v \theta} \frac 1 n \norm {\mathbf X \vbs \theta - \v y }_2^2 + \ld \norm{\vbs \theta }_1
$$

^7581f8

Lasso have no closed form solution


> [!NOTE] Important:
> $L^2$ pushes all parameters toward small value
> $L^1$ favor sparse solutions: few are non-zero, the rest are $0$ -> switch off some input $\implies$ use for feature selection

## General Explicit Regularization
$$
\hvbs \theta = \arg \min_{\vbs \theta } \underbrace{J(\vbs \theta; \mathbf X; \v y)}_\text{(i)} + \underbrace\ld_\text{(iii)} \cdot \underbrace{R(\vbs \theta)}_\text{(ii)}
$$
- (i) The cost function - encourage a good fit to the training data
- (ii) Regularization term - encourage small parameter values
- (iii) Regularization parameter $\ld$, determine the trade off between (i) and (ii)

## Elastic net
$$
R(\vbs \theta) = \norm{\vbs \theta}_1 + \norm{\vbs \theta}_2^2
$$
## Implicit Regularization
Early stoppng
Drop out ...
