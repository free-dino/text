If outliers in the data points in $\mathcal T$ only have a minor impact on the learned model, we say that the loss function is robust

From a statistical perspective, we can link the loss function to statistical properties of the learned model: 
- Maximum likelihood approach provides a formal connection between loss function and the probabilistic assumptions on the noise in the data
- Even for loss functions that are not derived from a likelihood perspective, we can relate to the asymptotic minimizer of the loss function to the statistical properties of the model

## Loss Functions for Regression
Squared error loss: $$L(y, \hat y) = (\hat y - y)^2$$
Assume $\epsilon$ from a Gaussian distribution $\epsilon \sim \mathcal N(0, \sigma_\epsilon^2)$. ([[1. Linear regression#^56ff6d|this]])
Absolute error loss: $$L(y, \hat y) = |\hat y - y|$$
Assume $\epsilon$ from a Laplace distribution $\epsilon \sim \mathcal L(0, b_\epsilon)$.  ([[Maximum likelihood for absolute error loss|this]])
The absolute error loss is more robust to outliers than the squared error loss since it grows more slowly for large error. ^a2940f

Huber loss:
$$
L (y, \hat y) = \begin{cases} \frac {1}{2}(\hat y - y^2) & \text {if } |\hat y - y|<1 \\ |\hat y - y| - \frac 1 2 & \text{otherwise}\end{cases}
$$
$\epsilon$-insensitive loss:
$$
L (y, \hat y) = \begin{cases} 0 & \text {if } |\hat y - y|<\epsilon \\ |\hat y - y| - \epsilon & \text{otherwise}\end{cases}
$$![[Pasted image 20240806125732.png]]

## Loss Functions for Binary Classification
Misclassification loss:
$$
L(y, \hat y) = \mathbb I\{\hat y \not  = y \} = \begin{cases} 
0 & \text{if } \hat y  = y \\
1 & \text{if } \hat y \not = y
\end{cases}
$$
This is rarely used in practice

Cross-entropy loss:
$$
L(y, g(\v x)) = \begin{cases} \ln g(\v x) & \text {if } y =1  \\
\ln(1-g(\v x)) & \text{if  } y = -1
\end{cases}
$$
More commonly used in practice

To define an entire family of loss functions, let define the concept of margins: 
Margin of classifier for a data points $(\v x, y)$ is $y \cdot f(\v x)$

We can re-formulate the logistic loss in terms of the margin as
$$
L(y \cdot f(\v x)) = \ln(1+ \exp(-y \cdot f(\v x)))
$$
Misclassification loss in margin terms:
$$
L(y \cdot f(\v x)) = \begin{cases} 1 & \text{if } y \cdot f(\v x) \le 1 \\ 0 & \text{otherwise} \end{cases}
$$
Exponential loss:
$$
L(y \cdot f(\v x)) = \exp(-y \cdot f(\v x))
$$
-> not robust against outlier

Hinge loss:
$$L(y \cdot f(\v x)) = \begin{cases} 1 -y \cdot f(\v x) & \text{if } y \cdot f(\v x) \le 1 \\ 0 & \text{otherwise} \end{cases}
$$
-> not a strictly proper loss function

Squared Hinge loss:
$$
L(y \cdot f(\v x)) = \begin{cases} (1 -y \cdot f(\v x))^2 & \text{if } y \cdot f(\v x) \le 1 \\ 0 & \text{otherwise} \end{cases}
$$
-> Less robust than hinge loss

Huberized squared hinge loss:
$$
L(y \cdot f(\v x)) = \begin{cases}
 -4y \cdot f(\v x) & \text{if } y \cdot f(\v x) \le - 1 \\
 ((1 -y \cdot f(\v x))^2) & \text {if  } -1 \le y \cdot f(\v x) \le 1, \quad \text{(squared hinge loss)}\\
0 & \text{otherwise} \end{cases}
$$
![[Pasted image 20240806133444.png]]