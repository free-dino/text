The more flexible a model is, the lower its bias will be.
But a model with low bias has a downside, that is overfitting or high variance.

By using such models as base models in bootstrap aggregating or bagging, we can reduce the variance of the model without using increasing its bias

## The bootstrap
The idea of bagging is to average over multiple base models, each learned from a slightly different training dataset.

The bootstrap: Artificially create multiple datasets of size $n$ out of one dataset also size $n$

Create $B$ dataset $\widetilde {\mathcal T}$ from the original dataset by randomly picking data points from $\mathcal T = \{ \v x_i, y_i\}^n_{i=1}$ 

-> The $\widetilde {\mathcal T}$ datasets are identically distributed but not independently distributed.

---
Algorithm: The bootstrap:

---
Data: Training dataset $\mathcal T = \{ \v x_i, y_i\}^n_{i=1}$
Result: Bootstrap data $\widetilde{\mathcal T} = \{\widetilde{\v x}_i, \widetilde y _i\}^n_{i=1}$   
1. For $i=1,\dots,n$ do:
	1. Sample $\ell$ uniformly on the set of integers $\{1, \dots, n\}$
	2. Set $\widetilde{\v x}_i$ and $\widetilde y_i = y_\ell$ 
	end
	

## Variance Reduction by Averaging
Average of $B$ models predictions
$$\hat y_\text{bag}(\v x_*)=\frac 1 B \sum_{b=1}^B \widetilde y^{(b)}(\v x_*) \ \ \ \ \text{or} \ \ \ \ \v g_\text{bag}(\v x_*) = \frac 1 B \sum_{b=1}^B \widetilde{\v g}^{(b)}(\v x_*)$$ Let $z_1, \dots, z_B$ be a collection of identically distributed (but possibly dependent) random variables with $\E[z_b]=\mu$ and $\text{Var}[z_b]=\sigma^2$ for $b=1,\dots,B$. Assume the average correlation between any pair of variables is $\rho$. Computing the mean and the variance of the average $\frac 1 B \sum_{b=1}^B z_b$ of these variables, we get:
$$
\begin{align}
\E &\Bigg [ \frac 1 B \sum_{b=1}^B z_b\Bigg] = \mu \\
\text{Var} &\Bigg [\frac 1 B \sum_{b=1}^B z_b \Bigg] = \frac{1-\rho}{B} \sigma^2 + \rho \sigma^2 
\end{align}
$$
[[Mean and variance of Bagging|proofs]] 
As $B \rightarrow \infty$ , the first term of the variance approach to $0$, but the variance will be limited by the correlation $\rho$ 

To summarize, by averaging the identically distributed predictions rom several base models, each with a low bias, the bias remain low and the variance is reduced.

More ensemble members does not make the resulting model more flexible, but only reduces the variance.

By the law of large numbers and the fact that the ensemble members are identically distribute, the bagging models becomes:
$$
\hat y_\text{bag}(x_*) = \frac 1 B \sum_{b=1}^B \widetilde y^{(b)} (x_*) \ce{->[$B \rightarrow \infty$]} \E\big[ \widetilde y ^{(b)}(x_*) \mid \mathcal T\big]
$$
As $B$ increase, we expect the bagging model to converge to the hypothetical (limited flexibility) model on the right hand side.

## Out-of-Bag Error Estimation
Always exist roughly 1/3 data points of the original training data points in $\mathcal T$. 

