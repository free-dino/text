[[1. Bagging|Bagging]] is an ensemble method for reducing the variance in high-variance base models.
Boosting in another ensemble method, primarily used for reducing bias in high bias model, i.e. [[3. A Rule-Based Method - Decision Trees#^4c3bad|classification tree]] of depth one (classification stump).


> [!NOTE] In summary
> Correct the errors produced by the previous models.



# AdaBoost

Classification: -1, +1
$$
\begin{align}
(\a^{(b)}, \hat y^{(b)}) &= \arg \min_{(\a, \hat y)} \sum_{i=1}^n L(y_i \cdot f^{(b)} (\v x_i)) \\
&= \arg\min_{(\a, \hat y)} \sum_{i=1}^n \exp\Big(-y_i \Big(f^{(b-1)}(\v x_i)+\a \hat y(\v x_i) \Big) \Big) \\
&= \arg\min_{(\a, \hat y)} \sum_{i=1}^n \underbrace{ \exp \Big( -y_if^{(b-1)} (\v x_i)\Big)}_{=w_i^{(b)}} \exp(-y_i\a \hat y{(\v x_i)}) \\
&= \arg\min_{(\a, \hat y)} \sum_{i=1}^n w_i^{(b)} \cdot
\begin{cases}
e^{-\a} & \text{if } y_i = \hat y (\v x_i) \\
e^{\a} & \text{if } y_i  \not = \hat y (\v x_i)
\end{cases}
 \\ &= \arg\min_{(\a, \hat y)} \Bigg ( e^{-\a} \underbrace { \sum_{i=1}^n w_i^{(b)} \mathbb I \set{y_i = \hat y{\v (x)}}}_{W_c} + e^{\a}\underbrace { \sum_{i=1}^n w_i^{(b)} \mathbb I \set{y_i \not = \hat y{\v (x)}}}_{W_e} \Bigg) \\
 &= \arg\min_{(\a, \hat y)} \Big( e^{-\a} W_c + +e^{\a} W_e\Big) \\
  &= \arg\min_{(\a, \hat y)} \Big( e^{-\a} \Big)
\end{align}
$$