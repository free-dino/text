[[2. A Distance Based Method - k-NN|k-NN]] partitions the input space into disjoint regions, and each region is associated with a certain (constant) prediction.

> [!NOTE] Intuition for decision trees
> Come up with a set of rules that defines the region explicitly
# Learning a [[1. Machine Learning Exemplified#^1efd4f|Regression]] Tree
The prediction $\hat y(\v x_*)$ from a regression tree is a piecewise constant function of the input $\v x_*$. We can write this mathematically as: $$\hat y(\v x_*) = \sum_{\ell=1}^L  \hat y _ \ell \mathbb I \set{\v x_* \in R_\ell},$$where $L$ is the total number of regions (leaf nodes) in the tree, $R_\ell$ is the $\ell$th region, and $\hat y_\ell$ is the constant prediction for the $\ell$th region. $\mathbb I \set{\v x \in R_\ell}$ is the indicator function, where $\mathbb I \set{\v x \in R_\ell}=1$ if $\v x \in R_\ell$ and $\mathbb I \set{\v x \in R_\ell}=0$ otherwise.

Learning the tree from data corresponds to finding suitable values for the parameters defining the function above:
1. The regions $R_\ell$
2. The constant predictions $\hat y_\ell$, $\ell = 1, \dots, L$
3. The total size of the tree $L$.

If we start by assuming that the shape of the tree, the partition $(L, \{R_\ell\}^L_{\ell=1})$, is known, then we can compute the constants $\{\hat y_\ell\}^L_{\ell=1}$ as the average of the training data points falling in each region:
$$
\hat y_\ell = \text{Average} \set{y_i : \v x_i \in R_\ell}$$
