
> [!NOTE] Intuition
> If the test data point $\v x_*$ is close to training data point $\v x_i$, then the prediction $\hat y (\v x_*)$ should be close to $y_i$

# The $k$-Nearest Neighbors Method

We define the set $\mathcal N_* = \{i : \v x_i \text{ is one of the } k \text{ training data points closest to } \v x_*\}$ and aggregate the information from the $k$ outputs $y_j$ for $j \in \mathcal N_*$ to make the prediction

For [[1. Machine Learning Exemplified#^1efd4f|regression problem]], we take the average of all $y_j$ for $j \in \mathcal N_*$

For [[1. Machine Learning Exemplified#^6e9242|classification problem]] we use a majority vote. Ties can be handled in different ways, for instance by a coin flip, or by reporting the actual vote count to the end user, who gets to decide what to do with it.

$k$-NN method is a non-parametric method.

---
**Data**: Training data $\{\v x_i, y_i\}^n _ {i=1}$ ans test input $\v x_*$
**Result**: Predicted test output $\hat y(\v x_*)$
1. Compute the distance $\norm{\v x_i - \v x_*}_2$ for all training data points $i=1,\dots, n$
2. Let $\mathcal N_* = \{i : \v x_i \text{ is one of the } k \text{ data points closest to } \v x_*\}$
3. Compute the $\hat y(\v x_*)$ as $$\hat y(\v x _*) = \begin{cases} \text{Average}\set{y_j: j\in \mathcal N_*} & (\text{Regression problems}) \\ \text{Majority Vote} \set{y_j : j \in \mathcal N_*} & (\text{Classification problems}) \end{cases}$$
---

# Decision Boundaries for a Classifier
Prediction might be the ultimate goal of the application, but in order to visualize and better understand a classifier, we can also study its decision boundary.

The boundary can only possible to visualize easily when the dimension of $\v x$ is $p = 2$

![[Pasted image 20250119150353.png]]

$k$-NN is a non-linear classifier

# Choosing $k$
$k$ is a hyperparameter, since it's chosen by the user

$k=1$: overfitting 
Big $k$: underfitting

Selecting $k$ is thus a trade-off between flexibility and rigidity. A systematic way of choosing a good value for $k$ is to use [[2. Estimating E_new#^c2d70c|cross-validation]]

# Input Normalization
The different ranges lead to a variable being considered much more important than others by $k$-NN.

To avoid this undesired effect, we can re-scale the input variables.

Option 1: Normalization $$x_{ij}^\text{new} = \frac{x_{ij} - \min_\ell(x_{\ell j})}{\max_\ell (x_{\ell j}) - \min_{\ell}(x_{ell j})}, \qquad \forall j=1,\dots,p, \quad i=1,\dots, n$$
Option 2: Standardization $$x_{ij}^{\text{new}} = \frac{x_{ij}-\bar x_j}{\sigma _j}, \qquad \forall j=1,\dots,p, \quad i=1,\dots, n$$
where $\bar x_j$ and $\sigma_j$ are the mean and standard deviation for each input variable respectively