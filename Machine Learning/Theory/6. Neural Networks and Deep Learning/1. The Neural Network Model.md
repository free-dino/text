[[1. Linear regression|Linear regression model]]:
$$
\hat y = W_1x_1 + W_2x_2 + \dots + W_px_p + b
$$

^c254be
$W$: weight 
$b$: off set term
![[Pasted image 20240810202202.png]]
$\v x$ has $(p+1)$ dimensions

To describe the non-linear relationship between $\v x$ and $\hat y$. We introduce non-linear scalar function $h: \R \mapsto \R$ called activate function. 

Now we modify [[#^c254be|this]] as generalized linear regression:
$$
\hat y = h(W_1x_1 + W_2x_2 + \dots + W_px_p + b)
$$
![[Pasted image 20240810202859.png]]

Common choices of activation function are : 
$$
\text{Logistic function : } h(z)= \frac {1}{1+e^{-z}}
$$
![[Pasted image 20240810203638.png]]
$$
\text{ReLU : } h(z)= \max(0,z)
$$
![[Pasted image 20240810203655.png]]
## Two-layer Neural Network
The parameters for the $k$th regression model are $b_k, W_{k1}, \dots, W_{kp}$ and we denote the output $q_k$,
$$
q_k = h(W_{k1}x_1 + W_{k2}x_2 + \dots + W_{kp}x_p+ b_k), \qquad k = 1, \dots, U.
$$
This has: $(p+1)$ parameters
There are $U$ outputs $\implies$ There are $U\cdot(p+1)$ parameters from $q_k$  
Those intermediate outputs $q_k$ are called hidden-units
The $U$ different hidden units $\{q_k\}^U_{k=1}$ act as input variables to an additional linear regression model:
$$
\hat y = W_1q_1 + W_2q_2 + \dots + W_Uq_U + b 
$$
This has $(U+1)$ parameters
This make the whole network has $U\cdot (p+1) + U + 1 = U\cdot(p+1) + 1$ parameters
The equations describing this two-layer neural network are:
$$
\begin{align}
q_1 &= h(W_{11}^{(1)}x_1 + W_{12}^{(1)}x_2 + \dots + W_{1p}^{(1)}x_p+ b^{(1)}_1), \\
q_2 &= h(W_{21}^{(1)}x_1 + W_{22}^{(1)}x_2 + \dots + W_{2p}^{(1)}x_p+ b^{(1)}_2), \\
& \vdots \\
q_U &= h(W_{U1}^{(1)}x_1 + W_{U2}^{(1)}x_2 + \dots + W_{Up}^{(1)}x_p+ b^{(1)}_U)
\end{align}
$$
$$
\hat y = W_1^{(2)}q_1+W_2^{(2)}q_2 + \dots + W_p^{(2)}q_p + b^{(2)}
$$

![[Pasted image 20240810213129.png]]

## Vectorization over Units
Weight matrix $\mathbf W$ and offset vector $\v b$

$$
\begin{align}
\mathbf W^{(1)} &= \begin{bmatrix} W_{11}^{(1)} & \cdots & W_{1p}^{(1)} \\ \vdots & & \vdots \\ W_{U1}^{(1)} & \cdots & W_{Up}^{(1)} \end{bmatrix},  &\v b^{(1)} = \begin{bmatrix}b_1^{(1)} \\ \vdots \\ b_U^{(1)} \end{bmatrix}, \\
\mathbf W^{(2)} & = \begin{bmatrix} W_1^{(2)} & \cdots & W_U^{(2)}\end{bmatrix}, &\v b^{(2)} = \begin{bmatrix}b^{(2)} \end{bmatrix} .
\end{align}
$$
$\mathbf W^{(1)} \in \R^{U \times p}$,
$\v b^{(1)} \in \R^{U \times 1}$,
$\mathbf W^{(2)} \in \R^{1 \times U}$,
$\v b^{(2)} \in \R^{1 \times 1}$
$\v x = \begin{bmatrix} x_1 & \cdots & x_p \end{bmatrix}^\top \implies \v x \in \R^{p \times 1}$
The full model can then be written as:
$$
\begin{align}
\v q &= h(\mathbf W^{(1)} \v x + \v b ^{(1)}), \\
\hat y & = \mathbf W^{(2)} \v q + \v b^{(2)},
\end{align}
$$
$\v q = \begin{bmatrix} q_1 & \cdots & q_U \end{bmatrix}^\top \implies \v q \in \R^{U \times 1}$
The 2 weight matrices and the 2 offset vectors are the parameters of the model, written as:
$$
\vbs \theta = \begin{bmatrix} \text{vec}(\mathbf W^{(1)})^ \top \\ \v b^{(1) \top} \\
\text{vec}(\mathbf W^{(2)})^ \top \\
\v b ^{(2)\top}
 \end{bmatrix}
$$
the operator $\text{vec}()$ takes all elements in the matrix and puts them into a vector.

## Deep Neural Network
We enumerate the layers with index $l \in \{1, \dots, L\}$ where $L$ is the number of layers. Each layer is a parametrized with a weight matrix $\mathbf W^{(l)}$ and an offset vector $\v b^{(l)}$. We also have multiple layers of hidden units denoted by $\v q^{(l)}$. Each such layer consists of $U_l$ hidden units $\v q^{(l)} = \begin{bmatrix} q_1^{(l)} \cdots q_{U_l}^{(l)} \end{bmatrix}^\top$, where the dimensions $U_1, \dots, U_{L-1}$ can be different across the various layers.

Each layer maps a hidden layer $\v q^{(l-1)}$ to the next hidden layer $q^{(l)}$ according to:
$$
\v q^{(l)} = h(\mathbf W^{(l)}\v q^{(l-1)} + \v b^{(l)})
$$
A deep neural network of $L$ layers can be described mathematically as:
$$
\begin{align}
\v q^{(1)} &= h(\mathbf W^{(1)}\v x + \v b^{(1)}), \\
\v q^{(2)} &= h(\mathbf W^{(2)}\v q^{(1)} + \v b^{(2)}), \\
&\ \ \vdots \\
\v q^{(L-1)} &= h(\mathbf W^{(L-1)}\v q^{(L-2)} + \v b^{(L-1)}), \\
\hat y &= h(\mathbf W^{(L)}\v q^{(L-1)} + \v b^{(L)})
\end{align}
$$
![[Pasted image 20240810222006.png]]

## Vectorization over Data Points
During training, the neural network model is used to compute the predicted output for several inputs $\{\v x_i\}^n_{i=1}$ 
For 2 layers neural network: 
$$
\begin{align}
\v q_i^\top & = h(\v x_i^\top \mathbf W^{(1)\top} + \v b^{(1)\top}), \\
\hat{y_i} &= \v q_i^\top \mathbf W^{(2)} + \v b^{(2)\top}, & i = 1,\dots, n 
\end{align}
$$
We have:
$$
\begin{align}
&
\v y = 
\begin{bmatrix} 
y_1 \\ \vdots \\ y_n
\end{bmatrix},
&
\mathbf X = 
\begin{bmatrix} 
\v x_1^ \top \\ \vdots \\ \v x_n^\top
\end{bmatrix},
\ \ \ \ \ \ \ \
\hvbs y =
\begin{bmatrix} 
\hat y_1 \\ \vdots \\ \hat y_n
\end{bmatrix},
\ \ \ \ \text{and} \ \ \ \
\mathbf Q =
\begin{bmatrix} 
\v q_1^\top \\ \vdots \\ \v q_n^\top
\end{bmatrix},
\end{align}
$$

The generalized problem can then conveniently written as:
$$
\begin{align}
\mathbf Q &= h(\mathbf X \mathbf W^{(1)\top} + \v b^{(1)\top}), \\
\hvbs y & = \mathbf Q \mathbf W^{(2)\top} +\v b ^{(2)\top}
\end{align}
$$

## Neural Networks for Classification
We extend neural network to [[2. Classification and Logistic Regression#^672047|logistic regression]] 
softmax parametrization: 
$$
\text{softmax}(\v z) \overset{\D}{=} \frac {1}{\sum_{j=1}^M e^{z_j}}
\begin{bmatrix} e^{z_1} \\ e^{z_2} \\ \vdots \\ e^{z_M} \end{bmatrix}
$$

the softmax function now becomes an additional activation function acting on the final layer of neural network: 
$$
\begin{align}
\v q^{(l)} &= h (\mathbf W^{(l)}\v x + \v b^{(l)}), \\
&\ \ \vdots \\
\v q^{(L-1)} &= h (\mathbf W^{(L-1)}\v q^{(L-2)} + \v b^{(L-1)}), \\
\v z &= \mathbf W^{(L)}\v q^{(L-1)} + \v b^{(L)}, \\
\v g &= \text{softmax}(\v z)
\end{align}
$$
$\text{softmax} : \v z \mapsto \v g$ 