$$
\vbs \theta = \begin{bmatrix} \text{vec}(\mathbf W^{(1)})^\top & \v b ^{(1)\top} & \cdots & \text{vec}(\mathbf W^{(L)})^\top & \v b ^{(L)\top}\end{bmatrix}
$$
To find suitable values for the parameters $\vbs \theta$, we solve an optimization problem the form
$$
\hvbs \theta = \arg \min_{\vbs \theta} J(\vbs \theta), \ \ \ \ \ \ \ \ \text{where } J(\vbs \theta) = \frac 1 n \sum_{i=1}^n L(\v x_i, y_i, \vbs \theta)
$$
For regression problem: 
$$
L(\v x, y, \vbs \theta) = (y-f(\v x; \vbs \theta))^2
$$
where $f(\v x; \vbs \theta)$ is the output of the [[1. The Neural Network Model|neural network]],
For multiclass classification problem, we use the cross-entropy loss function:
$$
L(\v x, y, \vbs \theta)=-\ln g_y(\v f(\v x; \vbs \theta)) = -z_y + \ln \sum_{j=1}^M e^{z_j}
$$
where $z_j = f_i(\v x ; \vbs \theta)$ is the $j$th logit - that is, the $j$th output of the last layer before softmax function $\v g(\v z)$.

In deep learning, we typically use various versions of gradient based search:
1. Pick an initialization $\vbs \theta_0$
2. Update the parameters as $\vbs \theta_{t+1} \leftarrow \vbs \theta_t -\gamma \nabla_{\vbs \theta} J(\vbs \theta_t)$ for $t = 0, 1, 2, \dots$
3. Terminate when some criterion is fulfilled, and take the last $\vbs \theta_t$ as $\hvbs \theta$
We mostly use stochastic gradient descent
## Backpropagation
To summarize, we want to find:
$$
d \mat W^{(l)} \overset{\D}{=} \nabla_{\partial \mat W^{(l)}} J(\vbs \theta) = \begin{bmatrix} \frac{\partial J(\vbs \theta)}{\partial \mat W_{11}^{(l)}} & \cdots & \frac{\partial J(\vbs \theta)}{\partial \mat W_{1, U}^{(l)}} \\
\vdots & & \vdots \\
\frac{\partial J(\vbs \theta)}{\partial \mat W_{U^{(l)}, 1}^{(l)}}&
\cdots & 
\frac{\partial J(\vbs \theta)}{\partial \mat W_{U^{(l)}, U^{(l-1)}}^{(l)}}
\end{bmatrix}
$$
and 
$$
d\v b^{(l)} \overset{\D}{=}\nabla_{\partial \v b^{(l)}}J(\vbs \theta) = \begin{bmatrix}\frac{\partial J(\vbs \theta)}{\partial \v b_1^{(l)}} \\ \vdots \\ \frac{\partial J(\vbs \theta)}{\partial \v b_{U^{(l)}}^{(l)}} \end{bmatrix}
$$
for all $l = 1, \dots, L$
The cost function $J(\vbs \theta)$ only includes the losses in the current mini-batch.
We can update:
$$
\begin{align}
\mat W^{(l)}_{t+1} & \leftarrow \mat W_{t}^{(l)} - \gamma d \mat W_{t}^{(l)} \\
\v b^{(l)}_{t+1} & \leftarrow \v b^{(l)}_t - \gamma d \v b^{(l)}_t
\end{align}
$$
To compute these gradients efficiently, backpropagation exploits the structure of the model, using the chain rule of calculus

First, we describe how backpropagation works for one single data point $(\v x, y)$, we later generalize this to multiple data points.

The algorithm consists of 2 steps:
- forward propagation
- backward propagation

#### Forward propagation
$$
\begin{align}
&\v q^{(0)} = \v x, \\
&\begin{cases}
\v z^{(l)} = \mat W^{(l)}\v q^{(l-1)} + \v b^{(l)} \\
\v q^{(l)} = h(\v z^{(l)})
\end{cases} \\
&\v z^{(L)}= \mat W^{(L)}\v q^{(L-1)} + \v b^{(L)} \\
& J(\vbs \theta) = 
\begin{cases}
(y-z^{(L)})^2, & \text{if regression}, \\
-z^{(L)}_y + \ln \sum_{j=1}^M e^{z_j^{(L)}} & \text{if classification}
\end{cases}
\end{align}
$$
Since we only consider one data point, the cost function $J(\vbs \theta)$ will only include one loss term.
#### Backward propagation
We introduce the gradient of the cost $J(\vbs \theta)$ with respect to the hidden units $\v z^{(l)}$ and $\v q^{(l)}$, given by:
$$
d \v z^{(l)} \overset{\D}{=} \nabla_{\v z^{(l)}} J(\vbs \theta) = \begin{bmatrix} \frac{\partial J(\vbs \theta)}{\partial z_1^{(l)}} \\ \vdots \\ \frac{\partial J(\vbs \theta)}{\partial z_{U^{(l)}}^{(l)}}\end{bmatrix}
$$
and 
$$
d \v q^{(l)} \overset{\D}{=} \nabla_{\v q^{(l)}} J(\vbs \theta) = \begin{bmatrix} \frac{\partial J(\vbs \theta)}{\partial q_1^{(l)}} \\ \vdots \\ \frac{\partial J(\vbs \theta)}{\partial q_{U^{(l)}}^{(l)}}\end{bmatrix}
$$

We compute the gradients $d \v z^{(l)}$ and $d \v q^{(l)}$ for all layers recursively, starting from last layer $L$, we propagate back to the first layer. 
If we have a regression problem and choose the squared error loss, we get:
$$
dz^{(L)} = \frac{\partial J(\vbs \theta)}{\partial z^{(L)}} = \frac{\partial}{\partial z^{(L)}} (y-z^{(L)})^2 = -2(y-z^{(L)}).
$$
For a multiclass classification problem, we instead use the cross-entropy loss, and we get
$$
dz_j^{(L)} = \frac{\partial J(\vbs \theta)}{\partial z_j^{(L)}} = \frac{\partial}{\partial z_j^{(l)}} \Big(-z_y^{(L)} + \ln \sum^M_{k=1} e^{z_k^{(L)}}\Big) = -\mathbb I\{ y=j\} + \frac{e^{z_j^{(L)}}}{\sum_{k=1}^M e^{z_k^{(L)}}}
$$
The backwards propagation now proceeds with the following recursions:
$$
\begin{align}
d\v z^{(l)} & = d \v q^{(l)} \odot h'(\v z^{(l)}) \\
d \v q^{(l-1)} &= \mat W^{(l)\top} d\v z^{(l)}
\end{align}
$$
where $\odot$ denotes the element-wise product and where $h'(z)$ is the derivative of the activate function.

With $d \v z^{(l)}$, we can now compute the gradient of the weight matrices and offset vectors as:
$$
\begin{align}
d \mat W^{(l)} &= d \v z^{(l)} \v q^{(l-1)\top} \\
d \v b^{(l)} & = d \v z^{(l)}
\end{align} ^07a150
$$
and here is the ([[Derivation of the Backpropagation Equations|this]])

We can visualize this as:
![[Pasted image 20240811224417.png]]

We want to compute the cost function $J(\vbs \theta)$ and it gradients $d\mat W^{(l)}$ and $d \v b^{(l)}$ for the whole mini-batch $\{ \v x_i, y_i\}^{jn_b}_{i=(j-1)n_b+1}$, so we run the forward propagation and backward propagation for all data points in the current mini-batch and average their result for $J$, $d\mat W^{(l)}$ and $d \v b^{(l)}$.

To do efficiently compute those thing, we process all $n_b$ data points in the mini-batch simultaneously by stacking them in matrices as we did in where each row represent one data point:
$$
\mat Q = \begin{bmatrix} \v q_1^\top \\ \vdots \\ \v q_{n_b}^\top \end{bmatrix}, \ \ \ \ \mat Z= \begin{bmatrix} \v z_1^\top \\ \vdots \\ \v z_{n_b}^\top\end{bmatrix}, \ \ \ \ d\mat Q = \begin{bmatrix} d \v q_1^\top \\ \vdots \\ d \v q_{n_b}^\top \end{bmatrix}, \ \ \ \ d \mat Z= \begin{bmatrix} d \v z_1^\top \\ \vdots \\ d \v z_{n_b}^\top\end{bmatrix}
$$
The Backpropagation Algorithm:

---
INPUT: Parameter $\vbs \theta = \{\mat W^{(l)}, \v b^{(l)}\}_{l=1}^L$, activation function $h$, and data $\mat X, \v y$ with $n_b$ rows, where each row corresponds to one data point in the current mini-batch
OUTPUT: $J(\vbs \theta)$, $\nabla_{\vbs \theta}J(\vbs \theta)$ of the current mini-batch.
1. *Forward propagation*
2. Set $\mat Q^0 \leftarrow \mat X$
3. for $l=1, \dots, L$ do:
	1. $\mat Z^{(l)} = \mat Q^{(l-1)} \mat W^{(l)\top} + \v b^{(l)\top}$
	2. $\mat Q^{(l)} = h(\mat Z^{(l)})$     // *Do not execute this line for last layer $l=L$*
	end

4. *Evaluate cost function*
	1. $J(\vbs \theta) = \frac 1 {n_b}\sum_{i=1}^{n_b} (y_i - Z_i^{(L)})^2$ 
	2. $d\mat Z^{(L)} = -2(\v y - \mat Z^{(L)})$
7. *Backward propagation*
8. for $l =  L, \dots, 1$ do
	1. $d \mat Z ^{(l)} = d \mat Q^{(l)} \odot h'(\mat Z^{(l)})$    //*Do not execute this line for last layer $l=L$*
	2. $d \mat Q^{(l-1)} = d \mat Z^{(l)} \mat W^{(l)}$
	3. $d \mat W^{(l)} = \frac 1 n_b d \mat Z^{(l) \top} \mat Q^{(l-1)}$
	4. $d b_j^{(l)} = \frac 1 {n_b} \sum_{i=1}^{n_b}d Z_{ij}^{(l)}$     $\forall j$
	end
9. $\nabla_{\vbs \theta} J(\vbs \theta) = \begin{bmatrix} \text{vec}(d \mathbf W^{(1)})^\top & d\v b ^{(1)\top} & \cdots & \text{vec}(d \mathbf W^{(L)})^\top & d \v b ^{(L)\top}\end{bmatrix}$
10. return $J(\vbs \theta), \nabla_{\vbs \theta}J(\vbs \theta)$
