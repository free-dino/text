Dropout is a bagging-like technique that allows us to combine many neural networks without the need to train them separately.

## Ensemble of Sub-networks
Randomly removing some of the hidden units -> Removing all its incoming and outgoing connections.

Mathematically, we can write this as sampling a mask $\v m^{(l-1)} = [m_1^{(l-1)} \ \ \ \ \cdots \ \ \ \ m_{U_{l-1}}^{(l-1)}]$ for each layer, multiplying that mask element-wise with the hidden units $\v q^{(l-1)}$ and then feeding the masked units $\tilde{\v q}^{(l-1)}$ to the next layer:
$$
\begin{align}
m_j^{(l-1)} &=
\begin{cases}
1 & \text{with probability } r \\
0 & \text{with probability } 1-r
\end{cases}
& \forall j=1,\dots, U_{(l-1)} \\
\tilde{\v q}^{(l-1)} &= \v m^{(l-1)} \odot \v q^{(l-1)} \\
\tilde{\v q}^{(l)} &= h(\mat W^{(l)} \tilde{\v q}^{(l-1)}+\v b^{(l)})
\end{align}
$$
$r$ is a hyperparameter set by the user. We can choose to apply to all layers or only some of them and can also use different probabilities $r$ for the different layers.
-> We just turn their weight into 0.
## Training with Dropout
Use stochastic gradient descent.
## Prediction at Test Time
