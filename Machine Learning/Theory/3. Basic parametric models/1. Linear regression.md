## The Linear Regression Model
Assume output variable $y$ (a scalar) can be described as an [[Affine space|affine]] combination of $\v x$ and a noise term $\epsilon$,
$$
\begin{align}
y &= \theta_0 + \theta_1x_1 + \theta_2+\dots+ \theta_px_p+\epsilon \\
& = \begin{bmatrix} \theta_0 & \theta_1 & \theta_2 & \cdots & \theta_p\end{bmatrix}
\begin{bmatrix} 1 \\ x_1 \\ x_2 \\ \vdots \\x_p\end{bmatrix} 
+\epsilon \\
& =\vbs \theta ^ \top \v x + \epsilon.
\end{align}
$$
$\v{\boldsymbol \theta}$ is the parameters vector with size $(p+1)$.
$\v x$ is the input vector with size $(p+1)$.

> [!Note] Learning of the model
> Learning of the model amounts to finding suitable values for $\boldsymbol \theta$ based on observer training data.

The goal is to predict:
$$
\pred y {x_*} = \hat \theta_0 + \hat \theta_1x_{*1}+\hat \theta_2x_{*2} + \dots + \hat \theta_px_{*p} =\vbs {\hat \theta} ^ \top \v x_*.
$$
## Training a Linear Regression Model from Training data
To train a linear regression model, that is to learn $\v {\boldsymbol \theta}$, from the training data $\mathcal T = \{\v x_i, y_i\}^n_{i=1}$.
We collect the data in the $n \times (p+1)$ matrix $\mathbf X$ and $n$-dimensional vector $y$,
$$

\mathbf X = 
\begin{bmatrix} \v x_1^\top \\ \v x_2^\top \\ \vdots \\ \v x_p^\top
\end{bmatrix}

, \qquad 

\v y = 
\begin{bmatrix} \v x_1^\top \\ \v x_2^\top \\ \vdots \\ \v x_p^\top
\end{bmatrix}

, \qquad
\v {\hat y} = 
\begin{bmatrix} \pred y {\v x_1} \\ \pred y {\v x_2} \\ \vdots \\ \pred y {\v x_p}
\end{bmatrix}
$$
$$
\implies \v y = \mathbf X\vbs \theta + \v{\boldsymbol \epsilon} \implies \v {\hat y} = \mathbf X \vbs \theta
$$
## Loss Functions and Cost Functions
Training a model is amounts to finding the parameter values that minimize the cost
$$
\hvbs \theta = \arg \min_{\vbs \theta} \underbrace{\frac 1 n \sum_{i=1}^n \overbrace{L(\pred y {\v x_i, \vbs \theta}, y_i)}^\text{loss function}}_{\text{cost function }J(\vbs \theta)}. 
$$
## Least Squares and the Normal Equations
Squared error loss function:
$$
L(\pred y {\v x; \vbs \theta}, y) = (\pred y {\v x; \vbs \theta} - y)^2
$$

^852d2e

if $\pred y {\v x; \vbs \theta} \not = y$, the function's values will increase quadratically.
Now we have t
he cost function:
$$
\begin{align}
J(\vbs \theta) &= \frac 1 n \sum_{i=1}^n L(\pred y {\v x_i, \vbs \theta} - y_i) \\
& = \frac 1 n \sum_{i=1}^n (\pred y {\v x_i, \vbs \theta} - y_i) \\
& = \frac 1 n \norm{\v {\hat y} - \v y}_2^2 \\
& = \frac 1 n \norm{\mathbf X \vbs \theta - (X \vbs \theta + \vbs \epsilon)}_2^2 \\
& = \frac 1 n \norm{-\vbs \epsilon}_2^2 = \frac 1 n \normvec {\boldsymbol \epsilon}_2^2
\end{align}
$$
$\implies J(\vbs \theta)$ is a convex function

Now we can attempt to optimize the cost function:
$$
\begin{align}
\hvbs \theta &= \arg \min_{\vbs \theta} J(\vbs \theta) \\
& = \arg \max \frac 1 n \norm {\mathbf X \vbs \theta- \v y}^2_2
\end{align}
$$
Because $\frac 1 n > 0 \quad, \forall n \in \mathbb N^*$, without loss of generality:
$$
\hvbs{\theta} = \arg \min_{\vbs \theta} \norm{\mathbf X \vbs \theta - \v y}_2^2
$$
Let: 
$$
f(\vbs \theta) = \norm{\mathbf X \vbs \theta - \v y}^2_2 = \vbs \theta^\top \mathbf X^\top \mathbf X \vbs \theta - 2 \v y^\top \mathbf X \vbs \theta + \vbs y^\top \v y
$$
$f(\vbs \theta)$ has a positive quadratic form
$$
\begin{align}
&\implies \frac {\partial}{\partial \vbs \theta} f(\vbs \theta) = 0 \\
& \iff 2 \mathbf X ^ \top \mathbf X \hvbs \theta - 2 \mathbf X ^\top \v y = 0 \\
& \iff \mathbf X^\top \mathbf X \vbs \theta = \mathbf X^\top \v y \qquad \text{(normal equation)}
\end{align}
$$
If $\mathbf X ^ \top \mathbf X$ is invertible
$$
\vbs \theta = (\mathbf X ^ \top \mathbf X)^ {-1} \mathbf X^\top \v y
$$

^db22da

If $\mathbf X ^ \top \mathbf X$ is not invertible (its columns are linear dependent) $\implies \det(\mathbf X ^ \top \mathbf X) =0$. We can not use normal equation to solve this, but there is still a solution for this problem using regularization:
$$
\vbs \theta = (\mathbf X ^ \top \mathbf X + \lambda \mathbf I)^{-1} \mathbf X^ \top \v y
$$
Ridge regression:
$$
\arg \min_{\vbs \theta} \frac 1 n \norm {\mathbf X \vbs \theta - \v y}_2^2 + \lambda \normvec{\boldsymbol \theta}_2^2
$$

^e7a7c6

## Maximum Likelihood Perspective

^56ff6d

Maximum likelihood:
$$
\vbs \theta = \arg \max_{\vbs \theta} p(\v y \mid \mathbf X; \vbs \theta)
$$
This equation means that we have to find the $\vbs \theta$ that make $p(\v y)$ max.
Assume the noise term $\epsilon$ are independent, each with a normal distribution:
$$
\epsilon \sim \mathcal N(0, \sigma_\epsilon^2) 
$$
Since $\v y = \mathbf X \vbs \theta + \vbs \epsilon$, and the noise term are independent, which implies all observed data points are independent, and $p(\v y \mid \mathbf X; \vbs \theta)$ can be factorize as:
$$
p(\v y \mid \mathbf X; \vbs \theta) = \prod_{i=1}^n p(y_i\mid \v x_i, \vbs \theta)
$$
Now we consider the normal distribution over the random variable $y_i$ with the mean of $y= \vbs \theta ^ \top \v x + \epsilon$
$$
\begin{align}
p(y_i \mid \v x_i, \vbs \theta) &= \mathcal N(y_i; \vbs \theta^\top \v x_i, \sigma_\epsilon^2) \\
& = \frac {1}{\sqrt{2 \pi \sigma_\epsilon^2}} \exp(-\frac{1}{2\sigma_\epsilon^2} (\vbs \theta ^ \top \v x_i - y_i)^2) \\
\end{align}
$$
with the $\exp()$ called, we should use $\ln()$ for numerical reason.
To maximize the likelihood of $\v y$ with respect to $\vbs \theta$ 
Since $\ln()$ is a monotonically increase function $\implies \arg \max f(x) \sim \arg \max \ln f(x)$
$$
\begin{align}
\ln p(\v y \mid \mathbf X; \vbs \theta) &= \sum_{i=1}^n \ln p(y_i\mid \v x_i, \vbs \theta) \\
& = \sum_{i=1}^n \ln (\frac {1}{\sqrt{2 \pi \sigma_\epsilon^2}} \exp(-\frac{1}{2\sigma_\epsilon^2} (\vbs \theta ^ \top \v x_i - y_i)^2)) \\
& = \sum_{i=1}^n (\ln(1) - \ln(\sqrt{2\pi\sigma_\epsilon^2}) -\frac{1}{2\sigma_\epsilon^2}(\vbs \theta ^ \top \v x_i - y_i)^2) \\
& = \sum_{i=1}^n (\frac {-1} 2 \ln(2\pi \sigma^2_\epsilon) - \frac{1}{2\sigma_\epsilon^2}(\vbs \theta ^ \top \v x_i - y_i)^2)) \\
& = \frac{-n}{2}\ln(2\pi \sigma^2_\epsilon) - \frac{1}{2\sigma_\epsilon^2} \sum_{i=1}^n (\vbs \theta ^ \top \v x_i - y_i)^2
\end{align}
$$
Removing the terms and factors independent of $\vbs \theta$ does not change the maximizing argument and we rewrite the equation as:
$$
\hvbs \theta = \arg \max_{\vbs \theta} p(\vbs y \mid \mathbf X; \vbs \theta) = \arg \max_{\vbs \theta}-\sum_{i=1}^n (\vbs \theta ^ \top \v x_i - y_i)^2 = \arg \min_{\vbs \theta} \frac 1 n \sum_{i=1}^n (\vbs \theta ^ \top \v x_i - y_i)^2
$$
## Categorical Input Variables
Dummy variables $x$ as:
$$ x =
\begin{cases}
0 & \text{if } A \\
1 & \text{if } B
\end{cases}
$$
For linear regression:
$$
y = \theta_0 + \theta_1 x_1 + \epsilon = \begin{cases}
\theta_0 + \epsilon & \text{if } A \\
\theta_0 + \theta_1 + \epsilon & \text{if } B
\end{cases}
$$

If the categorical variable takes more than 2 values, we can make a OHE.