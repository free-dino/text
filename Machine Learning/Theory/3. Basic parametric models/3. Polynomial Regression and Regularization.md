$$
y = \theta_0 + \theta_1 x + \theta_2 x^2 +\theta_3 x^3 + \dots + \epsilon = \vbs \theta ^ \top \v z
$$
With $\v z = \begin{bmatrix} 1 & x & x^2 & \cdots \end{bmatrix}$
This is still a [[1. Linear regression|linear models]]
Easily get overfitting if the data has noise.

Regularization.
$L^2$ regularization, aka [[1. Linear regression#^e7a7c6|Ridge regression]]
$$
\hvbs \theta = \arg \min_{\vbs \theta} \frac 1 n \norm{\mathbf X \vbs \theta - \v y}_2^2 + \lambda \norm{\vbs \theta}_2^2
$$

^035f18

In practice, it can be wise to exclude $\theta_0$, the intercept, from the regularization

The larger $\lambda$ is, the smaller $\vbs \theta$ get.
With $\lambda = 0$, the regularization has no effect, whereas $\lambda \rightarrow \infty$ will force all parameters $\hvbs \theta$ to $0$ 

We can also use this in [[2. Classification and Logistic Regression|logistic regression]]:
$$
\hvbs \theta = \arg \min_{\vbs \theta} \frac 1 n \sum_{i=1}^n \ln (1+ e^{-y_i\vbs \theta ^ \top \v x_i}) + \lambda \norm{\vbs \theta}_2^2
$$
It's common in practice to train [[2. Classification and Logistic Regression|logistic regression models]] using [[3. Polynomial Regression and Regularization#^035f18|this]] instead of [[2. Classification and Logistic Regression#^cbf77b|this]] 