2 type of problem:
Real-valued regression $(y \in \mathbb R)$ and classification $(y \in \{1, \dots, M\})$ 

We have to choose a suitable form to model the output distribution $p(y \mid \v x; \vbs \theta)$ 
Next we should computing a [[1. Linear regression|linear regression]] term $z = \vbs \theta ^ \top \v x$ and then letting the distribution depend on $z$ in some appropriate way.

For example: Poisson regression
$$
\text{Pois}(y; \lambda) = p(y\mid \v x) =  \frac{\lambda ^ y e^{-\lambda}}{y!} \qquad y=0, 1, 2, \dots
$$
$\iff \mathbb E[y \mid \v x] = \lambda$ 
Because $\lambda > 0$, we need a positive function to map $\v x$ into $\lambda$. We can choose:
$$
\lambda = e^{\vbs \theta ^ \top \v x}
$$
We choose this function because:
* $\exp()$ is a monotonically increase function
* We can easily use $\ln()$ to solve
So we have the model:
$$
p(y \mid \v x; \vbs\theta) = \text{Pois}(y;e^{\vbs \theta ^ \top \v x} )
$$
And we can write the conditional mean of the output as
$$
\mathbb E[y \mid \v x; \theta] = \phi^{-1}(z)
$$
where $z= \vbs \theta ^ \top \v x$ and $\phi(\mu) \overset{\Delta}{=} \log(\mu)$ 

Specifically, a generalized linear model consists of:
1. A choice of output distribution $p(y \mid \v x; \vbs \theta)$ from the exponential family of distributions: Gaussian, Bernoulli, Poisson, exponential, gamma, etc
2. A linear regression term $z = \vbs \theta ^ \top \v x$
3. A strictly increasing, so-called link function $\phi$ such that $\mathbb E[y \mid \v x; \theta] = \phi^{-1}(z)$

Map the [[1. Linear regression|linear regression]] output through the inverse of a link function to obtain the mean of the output.
$\mu$ is the mean of $p(y \mid \v x; \vbs \theta)$ $\iff \phi(\mu) = \vbs \theta ^ \top \v x$

With [[2. Classification and Logistic Regression|binary logistic regression]], the output distribution $p(y \mid \v x, \vbs \theta)$ [[2. Classification and Logistic Regression#^f84a18|is a Bernoulli distribution]] 
The logit is computed as $z = \vbs \theta ^ \top \v x$ 
The link function is given by: $\phi(\mu) = \log(\frac{\mu}{1-\mu})$

Since the generalized linear models are defined in terms of conditional distribution, we train the model by finding the parameter values such that the negative log-likelihood of the training data is minimized:
$$
\hvbs \theta = \arg \min_{\vbs \theta} \Big [ - \frac 1 n \sum_{i=1}^n \ln p(y_i \mid \v x_i; \vbs \theta)\Big]
$$
A regularization penalty can be added to the cost function.