## A Statistical View of the Classification Problem
Classification amounts to predicting the conditional class probabilities
$$
p (y=m\mid \v x)
$$
In words, $p(y=m \mid \v x)$ describe the probability for class $m$ given that we know the input $\v x$ 

A classifier:
* Predict classes
* Learn the probabilities $p(y \mid \v x)$

We train a model $g(\v x)$ for which
$$
p(y=1 \mid \v x) \text{ is modelled by } g(\v x)
$$
and 
$$
p(y=-1 \mid \v x) \text{ is modelled by } 1-g(\v x)
$$
Because $g(\v x)$ is a model for probability, so $0 \leq g(\v x) \leq 1$ for any $\v x$

For multiclass problem, we instead let the classifier return a vector-valued function $\v {g(\mathbf x)}$ where:
$$
\begin{bmatrix}
p(y=1 \mid \v x) \\
p(y=2 \mid \v x) \\
\vdots \\
p(y=M \mid \v x)
\end{bmatrix}
\text{ is modelled by } 
\begin{bmatrix}
g_1(\v x) \\
g_2(\v x) \\
\vdots \\
g_M(\v x)
\end{bmatrix}
= \v {g(\mathbf x)}
$$
We required that $g_m(\v x) \ge 0$ and $\normvec{g(x)}_1 = \sum_{m=1}^M |{g_m(\v x)}| = 1$ for any $\v x$.
## The Logistic Regression Model for Binary Classification

^d8b6dd

We wish to learn a function $g(\v x)$ that approximates the conditional probability of the probability of the positive class.

Start with [[1. Linear regression| linear regression]] without the noise term:
$$
z = \theta_0 + \theta_1 x_1 + \theta_2x_2 + \dots + \theta_p x_p = \vbs \theta^ \top \v x
$$

^762c14

This is $\v x \mapsto z \in \mathbb \R$, we need to squeeze $z$ to the interval $[0,1]$ by using the logistic function $h(z) = \frac{e^z}{1+ e^z}$, result in:
$$
g(\v x) = \frac{e^{\vbs \theta ^ \top \v x}}{1+ e^{\vbs \theta ^ \top \v x}}
$$

^354043
$$
\implies 1-g(\v x) = 1-\frac{e^{\vbs \theta ^ \top \v x}}{1+ e^{\vbs \theta ^ \top \v x}} = \frac 1 {1+ e^{\vbs \theta ^ \top \v x}} = \frac{e^{-\vbs \theta ^ \top \v x}}{1+ e^{-\vbs \theta ^ \top \v x}}
$$
This can be interpreted as a probability

Our job is to learn $\vbs \theta$

Bernoulli distribution:
$$
\begin{cases}
p(y=1 \mid \v x) = g(\v x) = \frac{e^{\vbs \theta ^\top \v x}}{1+e^{\vbs \theta ^\top \v x}} \\
p(y=-1 \mid \v x) = g(\v x) = \frac{e^{-\vbs \theta ^\top \v x}}{1+e^{-\vbs \theta ^\top \v x}}
\end{cases}
$$

^f84a18

$\implies \mathbb E[y \mid \v x] = p(y=1 \mid \v x)$ 

## Training the Logistic Regression Model by Maximum Likelihood
We will not be able to use the [[1. Linear regression#^db22da | normal equations]] for learning $\vbs \theta$ due to the nonlinearity of the logistic function

From a maximum likelihood perspective, assuming that the training data points are independent, learning a classifier amounts to solving
$$
\begin{align}
\hvbs \theta &= \arg \max_{\vbs \theta} p(\v y \mid \mathbf X; \vbs \theta)\\& = \arg \max_{\vbs \theta} \prod_{i=1}^n p(y_i \mid \mathbf X, \vbs\theta) \\
& = \arg \max_{\vbs \theta} \sum _{i=1}^n \ln p(y_i \mid \vbs x_i, \vbs \theta) \\
&= \arg \max_{\vbs \theta} \sum_{i=1}^n
\begin{cases}
\ln g(\v x_i, \vbs \theta) & \text{if }y_i = 1, \\
\ln(1-g(\v x_i; \vbs \theta)) & \text{if } y_i = -1 
\end{cases}
\end{align}
$$

^cbf77b

We use the negative log-likelihood as a the cost function in order to turn the maximization into an equivalent minimization problem, that is:
$$
\begin{align}
J(\vbs \theta) &= -\frac{1}{n} \sum_{i=1}^n \ln p(y_i \mid \v x_i, \vbs \theta) \\
& = - \frac 1 n \sum_{i=1}^n
\begin{cases}
\ln g(\v x_i, \vbs \theta) & \text{if }y_i = 1, \\
\ln(1-g(\v x_i; \vbs \theta)) & \text{if } y_i = -1 
\end{cases} \\
& = \frac 1 n \sum_{i=1}^n
\underbrace{
\begin{cases}
- \ln g(\v x_i, \vbs \theta) & \text{if }y_i = 1, \\
- \ln(1-g(\v x_i; \vbs \theta)) & \text{if } y_i = -1 
\end{cases}}_{\text{Binary cross-entropy loss } L(g(\v x_i, \vbs \theta), y_i)}
\end{align}
$$
It is not specific to logistic regression but can be used for any binary classifier that predict probabilities $g(\v x; \vbs \theta)$
And because we chose the labels to be $\{-1, 1\}$, for $y_i=1$ we can write:
$$
g(\v x; \vbs \theta) = \frac{e^{\vbs \theta^\top \v x_i}}{1+e^{\vbs \theta^\top \v x_i}} = \frac{e^{y_i\vbs \theta^\top \v x_i}}{1+e^{y_i\vbs \theta^\top \v x_i}}
$$
and for $y_i=-1$
$$
1-g(\v x; \vbs \theta) = \frac{e^{-\vbs \theta^\top \v x_i}}{1+e^{-\vbs \theta^\top \v x_i}} = \frac{e^{y_i\vbs \theta^\top \v x_i}}{1+e^{y_i\vbs \theta^\top \v x_i}}
$$
Hence we can rewrite the cost function compactly as:
$$
\begin{align}
J(\vbs \theta) = \frac 1 n \sum_{i=1}^n -\ln\frac{e^{y_i\vbs \theta^\top \v x_i}}{1+e^{y_i\vbs \theta^\top \v x_i}} &= \frac 1 n \sum_{i=1}^n -\ln \frac{1}{1+e^{-y_i\vbs \theta^\top \v x_i}}\\
&= \frac 1 n \sum_{i=1}^n \underbrace{\ln(1+e^{-y_i\vbs \theta^\top \v x_i})}_{\text{Logistic loss }L(\v x_i, y_i, \vbs\theta)} 
\end{align}
$$
Learning a logistic regression model thus amounts to solving
$$
\hvbs \theta = \arg \min_{\vbs \theta} \sum_{i=1}^n \ln (1+ e^{-y_i\vbs \theta^\top \v x_i})
$$

^0d4ab9

The problem has no closed form solution, so we have to use numerical optimization instead.
## Predictions and Decision Boundaries
To make a "hard" prediction for the test input $\v x_*$, let $\pred y {\v x_*}$ be the most probable class. For binary classification, we can express this as:
$$
\pred y {\v x_*} =
\begin{cases}
1 & \text{if } g(\v x) > r \\
-1 & \text{if } g(\v x) \le r
\end{cases}
$$

^243f6f

with decision threshold $r=0.5$

### Algorithm

**Learn** binary logistic regression

---
**Data**: Training data $\mathcal T = \{\v x; y_i \}^n_{i=1}$ with output classes $y = \{-1, 1\}$
**Result**: Learned parameter vector $\vbs \theta$
1. Compute $\hvbs \theta$ by solving [[#^0d4ab9| this]] numerically
---


**Predict** with binary logistic regression

---
**Data**: Learned parameter vector $\hvbs \theta$ and test input $\v x_*$
**Result**: Predict $\pred y {\v x_*}$
1. Compute $g(\v x_*)$ by [[#^354043|this]] 
2. If $g(\v x_*)>0.5$, return $\pred y {\v x_*}=1$, otherwise return $\pred y {\v x_*}=-1$
---

In some applications, however, it can be beneficial to explore different thresholds than $r = 0.5$.

The decision boundary for binary classification can be computed by solving the equation
$$
g(\v x)= 1-g(\v x)
$$
The solution are points in the input space for which the 2 classes are predicted to be equally probable. Therefore, these points lie on the decision boundary. For binary logistic regression, this means
$$
\frac{e^{\vbs \theta ^ \top \v x}}{1+ e^{\vbs \theta ^ \top \v x}} = \frac 1 {1+ e^{\vbs \theta ^ \top \v x}} \iff e^{\vbs \theta ^ \top \v x} = 1 \iff \vbs \theta ^ \top \v x = 0
$$

This mean the decision boundaries in logistic regression always have the shape of a hyperplane.

We can also compactly write [[#^243f6f|this]] with the threshold $r=0.5$ as
$$
\pred y {\v x_*} = \text{sign}(\vbs \theta ^ \top \v x_*)
$$


> [!NOTE] Linear classifier
> A classifiers whose decision boundaries are linear hyperplanes is a linear classifier.

The same arguments and constructions as above can be generalized to the multiclass setting. Predicting according to the most probable class then amounts to computing the prediction as
$$
\pred y {\v x_*} = \arg \max_m g_m(\v x_*)
$$
The decision boundaries will be given by a combination of $M-1$ hyperplanes for a multiclass logistic regression model.
## Logistic Regression for More Than Two Classes

^672047

In this problem we will have to design a vector-valued function $\v {g(x)}$ whose elements should be non-negative and sum to one.

$z_m = \vbs \theta_m^\top \v x \implies \v z \begin{bmatrix} z_1 & z_2 & \cdots & z_m\end{bmatrix}^ \top$
$$
\text{soft}\max (\v z)\overset{\Delta}{=} \frac{1}{\sum_{m=1}^M e^{z_m}}
\begin{bmatrix}
e^{z_1} \\ e^{z_2} \\ \vdots \\ e^{z_M}
\end{bmatrix}
$$

^8ff797

if $e^{z_m}$ value is small, then the softmax function will suppress its value to 0.
The model:
$$
\v {g(x)} = \text{softmax}(\v z), \qquad \text{where }\v z = \begin{bmatrix} \vbs \theta_1^\top \v x \\ \vbs \theta_2^\top \v x \\ \vdots \\ \vbs \theta_M^\top \v x \end{bmatrix}
$$
Equivalently, we have:
$$
g_m(\v x) = \frac {e^ {\vbs \theta_m^\top \v x}}{\sum_{j=1}^M e^ {\vbs \theta_m^\top \v x}}, \qquad m=1,\dots , M
$$
Cost function (to minimize a maximum likelihood of a function, add minus):
$$
\begin{align}
J(\vbs \theta) &= \frac 1 n p(\v y \mid \mathbf X; \vbs \theta) \\
& = - \frac 1 n \prod_{i=1}^n p(y = y_i \mid \v x_i, \vbs \theta) \\
& = - \frac 1 n \sum_{i=1}^n \ln p(y = y_i \mid \v x_i, \vbs \theta) \\
& = - \frac 1 n \sum_{i=1}^n \ln g_{y_i} (\v x_i; \vbs \theta)\\
& = \frac 1 n \sum_{i=1}^n \underbrace{-\ln g_{y_i}(\v x_i; \vbs \theta)}_{\text{Multiclass cross-entropy loss } L(\v {g}(\v x_i; \vbs \theta), y_i)}
\end{align}
$$
We training: 
$$
\hvbs \theta = \arg \min_{\vbs \theta} J(\vbs \theta)
$$
$$
\begin{align}
J(\vbs \theta) &= \frac 1 n \sum_{i=1}^n -\ln (\frac {e^ {\vbs \theta_m^\top \v x}}{\sum_{j=1}^M e^ {\vbs \theta_m^\top \v x}}) \\
& = \frac 1 n \sum_{i=1}^n(-\vbs\theta ^\top_{y_i} \v x_i + \ln \sum_{j=1}^M e^{\vbs \theta ^\top_j \v x_i})
\end{align}
$$