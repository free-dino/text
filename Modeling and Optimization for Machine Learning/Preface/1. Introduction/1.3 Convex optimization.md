A convex optimization problem is one of the form
$$
\begin{array}{llll}
&\text{minimize } &\qquad f_0(\v x) \\
&\text{subject to } &\qquad f_i(\v x) \leq b_i, \qquad i=1,\dots,m
\end{array}
\tag{1.8}
$$
where the functions $f_0,\dots,f_m: \R^n \to \R$ are convex, i.e., satisfy
$$
f_i(\a\v x + \b \v y) \le \a f_i(\v x) + \b f_i(\v y)
$$for all $x, y \in \R^n$ and all $\a, \b \in \R$ with $\a + \b = 1$, $\a \ge 0, \b \ge 0$. [[1.2 Least-squares and linear programming#^1a7d2e|The least-squares problem]] and [[1.2 Least-squares and linear programming#^aa02cb|linear programming problem]] are both special cases of the general convex optimization problem.
# 1.3.1 Solving convex optimization problems
There is in general no analytical formula for the solution of convex optimization problems, but there are very effective methods for solving them.

- Interior-point methods work very well in practice, and in some cases can be proved to solve the problem to a specified accuracy with a number of operations that does not exceed a polynomial of the problem dimensions.
- Ignoring any structure in the problem, each step requires on the order of $$\max\{n^3, n^2m, F\}$$ operations, where $F$ is the cost of evaluating the first and second derivatives of the objective and constraint functions $f_0, \dots, f_m$.
- 