A mathematical optimization problem, or just optimization problem has the form

$$
\begin{array}{llll}
&\text{minimize } &\qquad f_0(\v x) \\
&\text{subject to } &\qquad f_i(\v x) \leq b_i, \qquad i=1,\dots,m
\end{array}
\tag{1.1}
$$

^2e1ba9

- Vector $\v x = (x_1, \dots, x_n)$ is the optimization variable of the problem. 
- The function $f_0: \R^n \to \R$ is the objective function ^c683eb
- The functions $f_i : \R^n \to \R, \quad i=1,\dots,m,$ are the (inequality) construct functions
- The constants $b_1,\dots, b_m$ are the limits, or bounds, for the constraints
- A vector $\v x^*$ is called the optimal, or a solution of the problem [[#^2e1ba9|(1.1)]], if it has the smallest objective value among all vectors that satisfy the constraints: for any $\v z$ with $f_1(\v z) \leq b_1, \dots, f_m(\v z) \leq b_m$, we have $f_0(\v z) \geq f_0(\v x^*)$

The optimization problem [[#^2e1ba9|(1.1)]] is called a linear program if the objective and constraint functions $f_0,\dots, f_m$ are linear:
$$
f_i(\a \v x + \b \v y) = \a f_i(\v x) + \b f_i(\v y) \tag{1.2}
$$
for all $\v x, \v y \in \R^n$ and all $\a, \b \in \R$.

A convex optimization problem is one in which the objective and constraint functions are convex, which means they satisfy the inequality: ^0694ed
$$
f_i(\a \v x + \b \v y) \leq \a f_i(\v x) + \b f_i(\v y) \tag{1.3}
$$
for all $\v x, \v y \in \R^n$ and all $\a, \b \in \R$ with $\a + \b = 1$, $\a \geq 0, \b \geq 0$

# 1.1.1 Applications
Data fitting

# 1.1.2 Solving optimization problems
A solution method for a class of optimization problem is an algorithm that computes a solution of the problem to some given accuracy