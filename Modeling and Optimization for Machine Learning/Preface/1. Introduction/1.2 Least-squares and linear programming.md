A special subclasses of [[1.1 Mathematical optimization#^0694ed|convex optimization]]: least squares and linear programming

# 1.2.1 Least-squares problem

^1a7d2e

A least squares problem is an [[1.1 Mathematical optimization|optimization problem]] with no constraints (m=0) and an objective which is a sum of squares of terms of the form $\v a_i^\top \v x - b_i$:
$$
\begin{align}
&\text{minimize} &f_0(\v x) = \norm{\mat A \v x - \v b}_2^2 = \sum_{i=1}^k (\v a _i ^\top \v x - b_i)^2 
\end{align} \tag{1.4}
$$

^f5a51b

Here $\mat A \in \R^{k \times n}$ ($k \geq n$), $\v a _i^\top$ are the rows of $\mat A$ and the vector $\v x \in \R^n$ is the optimization variable
## Solving least-squares problems
The solution of a least-squares problem [[#^f5a51b|(1.4)]] can be reduced to solving a set of linear equations, 
$$
(\mat A ^ \top \mat A) \v x = \mat A ^ \top \v b
$$
so we have the analytical solution $\v x =  (\mat A ^ \top \mat A)^{-1} \mat A^\top \v b$ 

-> can be solved in a time approximately proportional to $n^2k$ with a known constant

## Using least-squares
- Basis for regression analysis, optimal control and many parameter estimation and data fitting methods.
- Recognizing an optimization problem as a least-squares problem is straightforward; we only need to verify that the [[1.1 Mathematical optimization#^c683eb|objective]] is a quadratic function (and then test whether the associated quadratic form is positive semidefinite)

In weighted least-squares, the weighted least-squares cost
$$
\sum_{i=1}^k w_i(\v a_i^\top \v x - b_i)^2
$$
where $w_1, \dots, w_k$ are positive, is minimized

Another technique in least-squares in regularization, the simplest case:
$$
\sum_{i=1}^k (\v a^\top _i \v x - b_i)^2 + \rho \sum_{i=1}^n x_i^2
$$
where $\rho > 0$. -> standard least squares problem. 

# 1.2.2 Linear programming

^aa02cb

The objective and all constraint functions are linear:
$$
\begin{array}{llll}
&\text{minimize } &\qquad \v c ^\top \v x \\
&\text{subject to } &\qquad \v a_i^\top \v x \leq b_i, \qquad i=1,\dots,m
\end{array}
\tag{1.5}
$$
Here the vectors $\v c$, $a_i, \dots, a_m \in \R^n$ and scalars $b_1,\dots, b_m \in \R$ are problem parameters that specify the objective and constraint functions.

## Solving linear programs
There are a variety of very effective methods for solving them, including Dantzig's simplex method, and the more recent interior-point methods described later in this book.

Complexity: $n^2 m$ assuming $m \geq n$

## Using linear programming
The Chebyshev approximation problem:

$$
\begin{array}{llll}
&\text{minimize } &\qquad \max_{i=1 \dots k} |\v a_i^\top \v x - b_i| \\
\end{array}
\tag{1.6}
$$
The objective function of Chebyshev approximation problem is not differentiable. 
This problem can be solved by solving the linear program:

$$
\begin{array}{llll}
&\text{minimize } &\qquad t \\
&\text{subject to } &\qquad \v a_i^\top \v x - t \leq b_i, \qquad i=1,\dots,k \\
& & \qquad - \v a_i^\top \v x - t \leq -b_i, \qquad i=1,\dots,k 
\end{array}
\tag{1.7}
$$
with variables $\v x \in \R^n$ and $t \in \R$.