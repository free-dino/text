Consider unconstrained smooth convex optimization
$$ \min_x f(x)$$
That is $f$ is convex and differentiable with $\text{dom}(f) = \mathbb R^n$. Denote optimal criterion value by $f^* = \min_x f(x)$, and a solution by $x^*$
Gradient descent: choose initial point $x^{(0)} \in \mathbb R^n$, repeat:
$$x^{(k)} = x^{(k-1)} - t_k \cdot\nabla f(x^{(k-1)}), \quad k=1,2,3,...$$
Stop at the same point ^032488

## Gradient descent interpretation
At each iteration, consider the expansion
$$f(y) \approx f(x) + \nabla f(x)^T(y-x) + \frac{1}{2t} \norm{y-x}^2_2$$
Quadratic approximation, replacing usual Hessian $\nabla ^2 f(x)$ by $\frac{1}{t}I$ 
$f(x) + \nabla f(x)^T(y-x) \qquad$ linear approximation $f$ 
$\qquad \frac{1}{2t} \norm{y-x}^2_2 \qquad$ proximity term to $x$, with weight 1/2t

Choose next point $y = x^+$ to minimize quadratic approximation:
$$x^+ = x - t \nabla f(x)$$
![[Pasted image 20240320204105.png]]
Blue point is $x$, red point is :
$$x^+ = \arg \min_y f(x)+ \nabla f(x)^T(y-x) + \frac{1}{2t} \norm{y-x}^2_2$$
## Fixed step size
Simply take $t_k = t$ for all $k = 1,2,3,...$, can diverge if $t$ is too big.
Convergence analysis later will give us a precise idea of "just right"

## Backtracking line search
Adaptively choose the step size is to use backtracking line search
* First fix parameters $0 < \beta <1$ and $0 < \alpha \le 1/2$ 
* At each iteration, start with $t= t_{init}$, and while $$ f(x-t \nabla f(x)) > f(x) - \alpha t \norm{\nabla f(x)}^2_2$$ shrink $t = \beta t$. Else perform gradient descent update $$x^+ = x - t \nabla f(x)$$
Simple and tends to work well in practice (further simplification: just take $\alpha = 1/2$) 
![[Pasted image 20240321222117.png]]

## Exact line search

We could also choose step to do the best we can along direction of negative gradient, called exact line search:
$$
t = \arg \min_{s\ge 0}
 f(x-s\nabla f(x))$$
 Usually not possible to do this minimization exactly
 Approximation to exact line search are typically not as efficient as backtracking, and it's typically not worth it.
## Convergence analysis
Assume that $f$ convex and differentiable, with $\text{dom}(f) = \mathbb R^n$ and additionally that $\nabla f$ is Lipschitz continuous with constant $L>0$,
$$
\norm{\nabla f(x) - \nabla f(y)}_2 \le L \norm{x-y}_2 \quad \text{for any }x,y
$$
or when twice differentiable: $\nabla^2 f(x) \preceq LI$  
> [! Theorem]
> Gradient descent with fixed step size $t \le 1/L$ satisfies
> $$
> f(x^{(k)}) - f^* \le \frac{\norm{x^{(0)} - x^*}_2^2}{2tk}
> $$
> and same result holds for backtracking, with $t$ replaced by $\beta / L$

We say gradient descent has convergence rate $O(1/k)$. That is, it finds $\epsilon$-suboptimal point in $O(1/\epsilon)$ iterations

## Analysis for strong convexity
Remember: strong convexity of $f$ means $f(x) - \frac{m}{2} \norm{x}^2_2$ is convex for some $m>0$ (when twice differentiable: $\nabla ^2 f(x) \succeq mI$) 
Assume Lipschitz gradient as before, and also strong convexity:

> [! Theorem]
> Gradient descent with fixed step size $t\le 2/(m+L)$ or with backtracking line search satisfies
> $$
> f(x^{(k)}) - f^* \le \gamma^k \frac{L}{2} \norm{x^{(0)} - x^*}_2^2
> $$
> where $0<\gamma<1$

Rate under strong convexity is $O(\gamma ^k)$, exponentially fast! That is, it finds $\epsilon$-suboptimal point in $O\Big(log(1/ \epsilon)\Big)$ iterations 
![[Pasted image 20240321224724.png]]

Called linear convergence

Objective versus iteration curve looks linear on semi-log plot


Important note: $\gamma = O(1-m/L)$. Thus we can write convergence rate as

$$ O \Big(\frac{L}{m}log(1/\epsilon)\Big)$$
Higher condition number $L/m \implies$ slower rate. This is not only true of in theory, very apparent in practice too.

### A look at the conditions

A look at the conditions for a simple problem, $f(\beta) = \frac{1}{2} \norm{y-X\beta}_2^2$

Lipschitz continuity of $\nabla f$:
* Recall this mean $\nabla^2 f(x) \preceq LI$
* As $\nabla^2 f(\beta) = X^T X$, we have $L = \lambda_{\max} (X^TX)$

Strong convexity of $f$:
* Recall this means $\nabla^2 f(x) \succeq mI$
* As $\nabla^2 f(\beta) = X^TX$, we have $m=\lambda_{\min}(X^TX)$
* If $X$ is wide ($X$ is $n \times p$ with $p>n$), then $\lambda_{\min}(X^TX) = 0$, and $f$ can't be strongly convex
* Even if $\sigma_\min(X) > 0$, can have a very large condition number $L/m = \lambda_\max(X^TX)/\lambda_\min(X^TX)$ 

## Practicalities
Stopping rule: stop when $\norm{\nabla f(x)}_2$ is small
* Recall $\nabla f(x^*) = 0$ at solution $x^*$
* If $f$ is strongly convex with parameter $m$, then $$\norm{\nabla f(x)}_2 \le \sqrt{2m\epsilon} \implies f(x) - f^* \le \epsilon$$

| Pros                                                | Cons                                                                |
| --------------------------------------------------- | ------------------------------------------------------------------- |
| Simple idea<br>Each iteration is cheap              | Can often be slow <br>=> aren't strongly convex or well-conditioned |
| Fast for well-conditioned, strongly convex problems | Can't handle nondifferentiable functions                            |

## Better approach?
Gradient descent has $O(1/\epsilon)$ convergence rate over problem class of convex, differentiable functions with Lipschitz gradients

First-order method: iterative method, which updates $x^{(k)}$ in
$$x^{(0)} + \text{span} \{\nabla f(x^{(0)}), \nabla f(x^{(1)}), ..., \nabla f(x^{(k-1)})\}$$

> [! Theorem (Nesterov)]
> For any $k \le (n-1)/2$ and any starting point $x^{(0)}$, there is a function $f$ in the problem class such that any first order method satisfies
> $$
> f(x^{(k)}) - f^* \ge \frac{3L \norm{x^{(0)} - x^*}_2^2}{32(k+1)^2}
> $$

Can attain rate $O(1/k^2)$ or $O(1/\sqrt{\epsilon})$ Answer: yes 

## Analysis for nonconvex case
Assume $f$ is differentiable with Lipschitz gradient, now nonconvex. 
Asking for optimality is too much. Let's settle for an $\epsilon$-substationary point $x$, which means $\norm{\nabla f(x)}_2 \le \epsilon$ 

> [Theorem]
> Gradient descent with fixed step size $t \le 1/L$ satisfies
> $$
\min_{i=0,...,k} \norm{\nabla f(x^{(i)})}_2 \le \sqrt{\frac{2(f(x^{(0)}) - f^*)}{t(k+1)}}
$$
Thus gradient descent has rate $O(1/ \sqrt{k})$, or $O(1/ \epsilon^2)$, even in the nonconvex case for the finding stationary points.
This rate cannot be improved (over class of differentiable functions with Lipschitz gradients) by any deterministic algorithm

## Gradient boosting
Given responses $y_i \in \mathbb R$ and features $x_i \in \mathbb R^p$, $i=1,...,n$
Want to construct a flexible (nonlinear) model for response based on features. Weighted sum of trees:
$$
u_i = \sum_{j=1}^{m} \beta_j \cdot T_j(x_i), \qquad i = 1,...,n
$$
Each tree $T_j$ inputs $x_i$, outputs predicted response. Typically trees are pretty short
![[Pasted image 20240322120609.png]]Pick a loss function $L$ to reflect setting. For continuous responses, e.g., could take $L(y_i,u_i) = (y_i-u_i)^2$
Want to solve
$$
\min_\beta \sum_{i=1}^{n} L \Big(y_i, \sum_{j=1}^{M} \beta_j \cdot T_j(x_i) \Big )
$$
Indexes all trees of a fixed size (e.g., depth = 5), so M is huge. Space is simply too big to optimize.

Gradient boosting: basically a version of gradient descent that is forced to work with trees

First think of optimization as $\min_u f(u)$, over predicted values $u$, subject to $u$ coming from trees

Start with initial model, a single tree $u^{(0)} = T_0$. Repeat:
* Compute negative gradient $d$ at latest prediction $u^{(k-1)}$, $$
d_i = - \Big[\frac{\partial L(y_i,u_i)}{\partial u_i} \Big] \Big|_{u_i = u_i^{(k-1)}} \quad , \qquad i=1,...,n
$$
* Find a tree $T_k$ that is close to $a$, i.e., according to $$
\min_{\text{trees } T} \sum_{i=1}^{n} (d_i - T(x_i))^2
  $$
  * Not hard to (approximately) solve for a single tree
  * Compute step size $\alpha_k$, and update our prediction: $$u^{(k)} = u^{(k-1)} + \alpha_k \cdot T_k$$
  Note: predictions are weighted sums of trees, as desired