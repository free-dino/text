$\newcommand{\norm}[1]{\left \lVert #1 \right\rVert}$
## Composite functions
Suppose
$$f(x) = g(x) + h(x)$$
* $g$ is convex, differentiable, $\text{dom}(g) = \mathbb R^n$
* $h$ is convex, not necessarily differentiable

If $f$ were differentiable, then gradient descent update would be:
$$
x^+ = x-t \cdot \nabla f(x)
$$
Recall motivation: minimize quadratic approximation to $f$ around $x$, replace $\nabla ^2 f(x)$ by $\frac{1}{t}I$,
$$
x^+ = \arg \min_x \underbrace{f(x)+\nabla f(x)^T(z-x) + \frac{1}{2t} \norm{z-x}^2_2}_{\bar f_t(z)}
$$
In our case $f$ is not differentiable, but $f = g + h$, $g$ differentiable. Why don't we make quadratic approximation to $g$, leave $h$ alone?

That is, update
$$
\begin{align}
x^+ & = \arg \min_z \bar g_t(z) + h(z) \\
&= \arg \min_z g(x) + \nabla g(x)^T(z-x) + \frac{1}{2t} \norm{z-x}^2_2 + h(z) \\
& = \arg \min_z \frac{1}{2t} \norm{z- \big (x-t \nabla g(x)\big)}_2^2 + h(z)
\end{align}
$$

$\frac{1}{2t} \norm{z-\big (x-t\nabla g(x)\big)}_2^2 \quad$ stay close to gradient update for $g$
$\qquad \qquad h(z) \qquad \qquad$     also make $h$ small

## Proximal gradient descent
Define proximal mapping:
$$
\text{prox}_{h,t}(x) = \arg \min_z \frac{1}{2t} \norm{x-z}_2^2 +h(z)
$$

Proximal gradient descent: choose initialize $x^{(0)}$, repeat:
$$
x^{(k)} = \text{prox}_{h,t_k} \big(x^{(k-1)} - t_k \nabla g(x^{(k-1)})\big), \quad k=1,2,3,...
$$
To make this update step look familiar, can rewrite it as
$$
x^{(k)} = x^{(k-1)} - t_k \cdot G_{t_k}(x^{(k-1)})
$$
where $G_t$ is the generalized gradient of $f$,
$$
G_t(x) = \frac{x- \text{prox}_{h,t} (x-t\nabla g(x))}{t}
$$
Key point is that $\text{prox}_{h,t}\big(\cdot\big)$ has a closed-form for many important functions $h$. Note:
* Mapping $\text{prox}_{h,t} (\cdot)$ doesn't depend on $g$ at all, only on $h$
* Smooth part $g$ can be complicated, we only need to compute its gradients

Convergence analysis: will be in terms of the number of iterations and each iteration evaluates $\text{prox}_{h,t}(\cdot)$ once (this can be cheap or expensive, depending on $h$)

## Example: ISTA
Given $y \in \mathbb R^n$, $X \in \mathbb R^{n \times p}$, recall the lasso criterion:
$$
f(\beta) = \underbrace{\frac{1}{2} \norm{y-X\beta}_2^2}_{g(\beta)} + \underbrace{\lambda \norm{\beta}_1}_{h(\beta)}
$$
Proximal mapping is now
$$
\begin{align}
\text{prox}_t(\beta) &= \arg \min_z \frac{1}{2t} \norm{\beta - z}_2^2 + \lambda \norm{z}_1 \\
&= S_{\lambda t} (\beta)
\end{align}
$$
where $S_\lambda (\beta)$ is the soft-thresholding operator,
$$
[S_\beta(\lambda)]_i = 
\begin{cases}
\beta_i-\lambda & \text{if } \beta_i > \lambda \\
0 & \text{if } -\lambda \le \beta_i \le \lambda, \quad i=1,...,n \\
\beta_i + \lambda & \text{if } \beta_i < -\lambda
\end{cases}
$$
Recall $\nabla g(\beta) = -X^T(y-X\beta)$, hence proximal gradient update is:
$$
\beta^+ = S_{\lambda t} \big(\beta+t X^T(y-X\beta)\big)
$$
Often called the iterative soft-thresholding algorithm (ISTA). Very simple algorithm
![[Pasted image 20240324125117.png]]

## Backtracking line search
Backtracking for prox gradient descent works similar as before (in gradient descent), but operates on $g$ and not $f$

Choose parameter $0<\beta<1$. At each iteration, start at $t= t_\text{init}$, and while
$$
g\big(x-tG_t(x)\big) > g(x) - t\nabla g(x)^T G_t(x) + \frac{t}{2} \norm{G_t(x)}_2^2
$$
shrink $t=\beta t$, for some $0<\beta<1$. Else perform proximal gradient update

(Alternative formulations exist that require less computation, i.e., fewer calls to prox)

## Convergence analysis
For criterion $f(x) = g(x) + h(x)$, we assume:
* $g$ is convex, differentiable, $\text{dom}(g) = \mathbb R^n$, and $\nabla g$ is Lipschitz continuous with constant $L>0$
* $h$ is convex, $\text{prox}_t(x) = \arg \min_z \{\norm{x-z}_2^2/2t + h(z)\}$ can be evaluated

>[!Theorem]
>Proximal gradient descent with fixed step size $t \le 1/L$ satisfies: $$f(x^{(k)}) - f^* \le \frac{\norm{x^{(0)} - x^*}_2^2}{2tk}$$
>and same result holds for backtracking, with $t$ replaced by $\beta/L$

Proximal gradient descent has convergence rate $O(1/k)$ or $O(1/\epsilon)$.
Matches gradient descent rate! (But remember prox cost ...)

## Example: matrix completion
Given matrix $Y \in \mathbb R^{m \times n}$, and only observe entries $Y_{i,j}$, $(i,j) \in \Omega$. Suppose we want to fill in missing entries (e.g., for a recommender system), so we solve a matrix completion problem:
$$
\min_B \frac{1}{2} \sum_{(i,j) \in \Omega} (Y_{ij} - B_{ij})^2 + \lambda \norm{B}_{tr}
$$
Here $\norm{B}_{tr}$ is the trace (or nuclear) norm of $B$,
$$
\norm{B}_{tr} = \sum_{i=1}^r \sigma_i(B)
$$
where $r= \text{rank}(B)$ and $\sigma_1(X) \ge ... \ge \sigma_r(X)\ge 0$ are the singular values
Define $P_\Omega$, projection operator onto observed set:
$$
[P_\Omega(B)]_{ij} = 
\begin{cases}
B_{ij} & (i,j) \in \Omega \\
0 & (i,j) \not \in \Omega
\end{cases}
$$
Then the criterion is:
$$
f(B) = \frac{1}{2} \underbrace{\norm{P_\Omega(Y) - P_\Omega(B)}_F^2}_{g(B)} + \underbrace{\lambda \norm{B}_{tr}}_{h(B)}
$$
Two ingredients needed for proximal gradient descent:
* Gradient calculation: $\nabla g(B) = -(P_\Omega(Y) - P_\Omega(B))$
* Prox function: $$\text{prox}_t(B) = \arg \min_Z \frac{1}{2t} \norm{B-Z}^2_F + \lambda \norm{Z}_{tr}$$
Claim: $\text{prox}_t(B)= S_{\lambda t}(B)$, matrix soft-thresholding at the level $\lambda$. Here $S_\lambda(B)$ is defined by
$$
S_\lambda(B) = U\Sigma_\lambda V^T
$$
where $B=U\Sigma V^T$ is and SVD (Singular value decomposition) and $\Sigma_\lambda$ is a diagonal with
$$
(\Sigma_\lambda)_{ii} = \max\{\Sigma_{ii} - \lambda,0\}
$$
Proof: note that $\text{prox}_t(B)=Z$, where $Z$ satisfies
$$
0 \in Z -B + \lambda t \cdot\partial\norm{Z}_{tr}
$$
Helpful fact: if $Z = U \Sigma V^T$, then
$$
\partial \norm{Z}_{tr} = \{UV^T + W : \norm{W}_{op} \le 1, U^T W = 0, WV = 0\}
$$
Now plug in $Z= S_{\lambda t}(B)$ and check that we can get 0
Hence proximal gradient update step is:
$$
B^+ = S_{\lambda t} \Big(B+ t\big(P_\Omega(Y) - P_\Omega(B)\big) \Big)
$$
Note that $\nabla g(B)$ is Lipschitz continuous with $L=1$ so we an choose fixed step size $t=1$. Update step is now:
$$
B^+ = S_{\lambda} \big( P_\Omega (Y) + P_\Omega^\bot(B)\big)
$$
where $P_\Omega^\bot$ projects onto unobserved set $P_\Omega (B) + P_\Omega^\bot(B) = B$

This is soft-impute algorithm, simple and effective method for matrix completion

## Special cases
Proximal gradient descent also called composite gradient descent, or generalized gradient descent

There are several special cases, when minimizing $f = g+h$:
* $h=0$: gradient descent
* $h=I_C$: projected gradient descent
* $g=0$: proximal minimization algorithm

## Projected gradient descent
Given closed convex set $C \in \mathbb R^n$,
$$
\min_{x\in C} g(x) \iff \min_x g(x) + I_C(x)
$$
where $I_C(x) = \begin{cases} 0 & x\in C \\ \infty & x \not \in C\end{cases} \quad$ is there the indicator function of $C$
Hence
$$
\begin{align}
\text{prox}_t(x) &= \arg \min_z \frac{1}{2t} \norm{x-z}_2^2 + I_C(z) \\
&= \arg \min_{z \in C} \norm{x-z}^2_2
\end{align}
$$That is, $\text{prox}_t(x) = P_C(x)$, projection operator onto $C$

Therefore proximal gradient update step is:
$$
x^+ = P_C\big(x-t\nabla g(x)\big)
$$
That is, perform usual gradient update and then project back onto $C$. Called projected gradient descent

![[Pasted image 20240324215624.png]]

## Proximal minimization algorithm

Consider for $h$ convex (not necessarily differentiable),
$$\min_x h(x)$$
Proximal gradient update step is just:
$$
x^+ = \arg \min_z \frac{1}{2t} \norm{x-z}_2^2 + h(z)
$$
Called proximal minimization algorithm. Faster than subgradient method, but not implementable unless we know prox in closed form

## What happens if we can't evaluate prox?
Theory for proximal gradient, with $f = g+h$, assumes that prox function can be evaluated, i.e., assumes the minimization
$$\text{prox}_t(x) = \arg \min_z \frac{1}{2t} \norm{x-z}_2^2 + h(z)$$
can be done exactly. In general, not clear what happens if we just minimize this approximately.

But, if you can precise control the errors in approximating the prox operator, then you can recover the original convergence rates

In practice, if prox evaluation is done approximately, then it should be done to decently high accuracy

## Acceleration
We can accelerate proximal gradient descent in order to achieve the optimal $O(1/\sqrt \epsilon)$ convergence rate.

## Acceleration proximal gradient method
As before, consider:
$$\min_x g(x) + h(x)$$
where $g$ convex, differentiable, and $h$ convex. Accelerated proximal gradient method: choose initial point $x^{(0)} = x^{(k-1)} \in \mathbb R^n$, repeat:
$$
v = x^{(k-1)} + \frac{k-2}{k+1} (x^{(k-1)} - x^{(k-2)})
$$
$$x^{(k)} = \text{prox}_{t_k} (v - t_k \nabla g(v))$$
for $k = 1,2,3,...$
* First step $k=1$ is just usual proximal gradient update
* After that, $v= x^{(k-1)} + \frac{k-2}{k+1} (x^{(k-1)} - x^{(k-2)})$ carries some "momentum" from previous iterations
* When $h=0$ we get accelerated gradient method
![[Pasted image 20240324221750.png]]

![[Pasted image 20240324221820.png]]

## Backtracking line search

Backtracking under with acceleration in different ways. Simple approach: fix $\beta < 1$, $t_0 = 1$. At iteration $k$, start with $t = t_{k-1}$, and while
$$
g(x^+) > g(v) + \nabla g(v)^T (x^+ - v) + \frac{1}{2t} \norm{x^+ - v}_2^2
$$
shrink $t = \beta t$, and let $x^+ = \text{prox}_t(v-t\nabla g(v))$. Else keep $x^+$

Note that this strategy forces us to take decreasing step sizes ...
(more complicated strategies exist which avoid this)

## Convergence analysis
For criterion $f(x) = g(x) + h(x)$, we assume as before:
* $g$ is convex, differentiable, $\text{dom}(g) = \mathbb R^n$, and $\nabla g$ is Lipschitz continuous with constant $L>0$
* $h$ is convex, $\text{prox}_t(x) = \arg \min_z \{\norm{x-z}_2^2/(2t) + h(z)\}$ can be evaluated
> [! Theorem]
> Accelerated proximal gradient method with fixed step size $t\le 1/L$ satisfies $$f(x^{(k)}) - f^* \le \frac{2 \norm{x^{(0)} - x^*}_2^2}{t(k+1)^2}$$
> and same result holds for backtracking, with $t$ replaced by $\beta /L$

Achieves optimal rate $O(1/k^2)$ or $O(1/\sqrt \epsilon)$ for first-order methods

## FISTA
Back to lasso problem:
$$
\min_{\beta} \frac{1}{2} \norm{y-X\beta}_2^2 + \lambda \norm{\beta}_1
$$
Recall ISTA (Iterative Soft-thresholding Algorithm):
$$\beta ^{(k)} = S_{\lambda t_k}\big (\beta ^{(k-1)} + t_k X^T(y-X\beta^{(k-1)}) \big)$$
