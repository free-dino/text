Consider $f$ convex, having $\text{dom} (f) = \mathbb R^n$, but not necessarily differentiable
$\newcommand{\norm}[1]{\left \lVert #1 \right\rVert}$
Subgradient method: like gradient descent, but replacing gradients with subgradients, Initialize $x^{(0)}$, repeat:
$$x^{(k)} = x^{(k-1)} - t_k \cdot g^{(k-1)}, \qquad k=1,2,3,...$$
where $g^{(k-1)} \in \partial f(x^{(k-1)})$, any subgradient of $f$ at $x^{(k-1)}$
Subgradient method is not necessarily a descent method, thus we keep track of best iterate $x_{\text{best}}^{(k)}$ among $x^{(0)},..., x^{(k)}$ so far, i.e.,
$$f(x^{(k)}_{\text{best}}) = \min_{i=0,...,k} f(x^{(i)})$$
## Step size choices
* Fixed step sizes: $t_k = t$ all $k = 1,2,3,...$
* Diminishing step sizes: choose to meet conditions $$\sum_{k=1}^\infty t_k^2 < \infty, \quad \sum_{k=1}^\infty t_k = \infty$$ i.e., square summable but not summable. Important here that step sizes go to 0, but not to fast
There are several other options too, but key difference to gradient descent: step sizes are pre-specified, not adaptively computed

## Convergence analysis
Assume that $f$ convex, $\text{dom}(f)= \mathbb R^n$, and also that $f$ is Lipschitz continuous with constant $G>0$, i.e.,
$$|f(x) - f(y)| \le G \norm{x-y}_2 \quad \forall \space x,y$$
> [!Theorem]
> For a fixed step size $t$, subgradient method satisfies $$\lim_{k \rightarrow \infty f(x_{\text{best}}^{(k)})} \le f^* + G^2t/2$$

>[!Theorem]
>For a diminishing step sizes, subgradient method satisfies $$\lim_{k \rightarrow \infty} f(x_{\text{}best}^{(k)}) = f^*$$

## Basic inequality
Can prove both result from the same inequality. Key steps:
* Using definition of subgradient, $$\norm{x^{(k)}-x^*} \le \norm{x^{(k-1)}-x^*}_2^2 - 2t_k \big (f(x^{(k-1)}) - f(x^*)\big) + t^2_k \norm{g^{(k-1)}}_2^2$$
* Iterating last inequality, $$\norm{x^{(k)}-x^*}_2^2 \le \norm{x^{(0)} - x^*}_2^2 - 2 \sum_{i=1}^k t_i\big (f(x^{(i-1)}) -f(x^*)\big) + \sum_{i=1}^k t_i^2 \norm{g^{(i-1)}}_2^2$$
* Using $\norm{x^{(k)}-x^*}_2 \ge 0$, and letting $R = \norm{x^{(0)} - x^*}_2$, $$0 \le R^2-2\sum_{i=1}^kt_i\big(f(x^{(i-1)}) - f(x^*)\big) + G^2\sum_{i=1}^k t_i^2$$
* Introducing $f(x_{\text{best}}^{(k)}) = \min_{i=0,...,k}f(x^{(i)})$, and rearranging, we have the basic inequality $$f(x^{(k)}_{\text{best}}) - f(x^*) \le \frac{R^2 + G^2\sum_{i=1}^k t_i^2}{2\sum_{i=1}^kt_i}$$
For different step sizes choices, convergence result can be directly obtained from this bound, e.g., previous theorems follow

## Convergence rate
The basic inequality tells us that after $k$ steps, we have:
$$f(x^{(k)}_{\text{best}}) - f(x^*) \le \frac{R^2 + G^2\sum_{i=1}^k t_i^2}{2\sum_{i=1}^kt_i}$$
With fixed step size $t$, this gives
$$f(x^{(k)}_{\text{best}}) - f(x^*) \le \frac{R^2}{2kt} + \frac{G^2t}{2}$$
For this to be $\le \epsilon$, let's make each term $\le \epsilon/2$. So we can choose $t=\epsilon/G^2$, and $k=R^2/t \cdot 1/\epsilon = R^2G^2/\epsilon^2$
That is, subgradient method has convergence rate $O(1/\epsilon^2)$ ... note that this is slower than $O(1/\epsilon)$ rate of gradient descent

## Example regularize logistic regression
Given $(x_i,y_i) \in \mathbb R^n \times \{0,1\}$ for $i=1,...,n$, the logistic regression loss is
$$f(\beta) = \sum_{i=1}^n \Big(- y_ix_i^T\beta + \log(1+ \exp(x_i^T\beta))\Big)$$
This is a smooth an convex function with
$$\nabla f(\beta) = \sum_{i=1}^n \big(y_i - p_i(\beta)\big)x_i$$
where $p_i(\beta) = \exp(x_i^T \beta) / (1+\exp(x_i^T \beta))$, $i=1,...,n$. Consider regularized problem:
$$\min_{\beta} f(\beta) + \lambda \cdot P(\beta)$$ where $P(\beta) = \norm{\beta}_2^2$, ridge penalty; $P(\beta) = \norm{\beta}_1$, lasso penalty
Ridge: use gradients
Lasso: use subgradients
Example here has $n=1000$, $p=20$
![[Pasted image 20240323224841.png]]Step sizes hand-tuned to be favorable for each method (of course comparison is imperfect but it reveals the convergence behaviors)

## Polyak step sizes
Polyak step sizes: when the optimal value $f^*$ is known, take
$$
t_k = \frac{f(x^{(k-1)})-f^*}{\norm{g^{(k-1)}}_2^2}, \qquad k=1,2,3,...
$$
Can be motivated from first step n subgradient proof:
$$
\norm{x^{(k)}-x^*} \le \norm{x^{(k-1)} - x^*}_2^2 - 2t_k\big(f(x^{(k-1)}) - f(x^*)\big) + t_k^2\norm{g^{(k-1)}}_2^2
$$
Polyak step size minimizes the right-hand side
With Polyak step sizes, can show subgradient method converges to optimal value. Convergence rate is still $O(1/\epsilon^2)$

## Example intersection of sets
Suppose we want to find $x^* \in C_1 \cap ... \cap C_m$, i.e., find a point in intersection of closed, convex sets $C_1,...,C_m$

First define
$$
\begin{align}
&f_i(x) = \text{dist}(x,C_i), \quad i=1,...,m \\
&f(x) = \max_{i=1,...,m} f_i(x)
\end{align}
$$
and now solve
$$\min_x f(x)$$
Check: is this convex?
Note that $f^*=0 \iff x^* \in C_1 \cap ... \cap C_m$  

Recall the distance function $\text{dist}(x,C) = \min_{y \in C} \norm{y-x}_2$. Last time we computed its gradient
$$
\nabla \text{dist}(x,C) = \frac{x-P_C(x)}{\norm{x-P_C(x)}_2}
$$
where $P_C(x)$ is the projection of $x$ onto $C$
Also recall subgradient rule: if $f(x) = \max_{i=1,...,m} f_i(x)$, then
$$
\partial f(x) = \text{conv} \Big(\bigcup_{i: f_i(x)= f(x)} \partial f_i(x) \Big)
$$
So if $f_i(x) = f(X)$ and $g_i \in \partial f_i(x)$, then $g_i \in \partial f(x)$
Put these two facts together for intersection of sets problem, with $f_i(x) = \text{dist}(x,C_i):$ if $C_i$ is farthest set from $x$ (so $f_i(x) = f(x)$), and
$$
g_i = \nabla f_i(x) = \frac{x-P_{C_i}(x)}{\norm{x- P_{C_i}(x)}_2}
$$
then $g_i \in \partial f(x)$
Now apply subgradient method, with Polyak size $t_k = f(x^{(k-1)})$. At iteration $k$, with $C_i$ farthest from $x^{(k-1)}$, we perform update
$$
\begin{align}
x^{(k)} &= x^{(k-1)} - f(x^{(k-1)}) \frac{x^{(k-1)} - P_{C_i}(x^{(k-1)})}{\norm{x^{(k-1)} - P_{C_i}(x^{k-1})}_2} \\
&= P_{C_i}(x^{(k-1)})
\end{align}
$$

For two sets, this is the famous alternating projections algorithm, i.e., just keep projecting back and forth
![[Pasted image 20240323231849.png]]

## Projected subgradient method
To optimize a convex function $f$ over a convex set $C$,
$$
\min_x f(x) \quad \text{subject to } x\in C
$$
we can use the projected subgradient method. Just like the usual subgradient method, except we project onto $C$ at each iteration:
$$x^{(k)} = P_C\big(x^{(k-1)} - t_k \cdot g^{(k-1)}\big), \quad k=1,2,3,...$$
Assuming we can do this projection, we get the same convergence guarantees as the usual subgradient method, with the same step size choices.

What sets $C$ are easy to project onto? Lots, e.g.,
* Affine images: $\{Ax+b : x \in \mathbb R^n\}$
* Solution set of linear system: $\{x:Ax = b\}$
* Nonnegative orthant: $\mathbb R^n_+ = \{x: x \ge 0\}$
* Some norm balls: $\{x: \norm{x}_p \le 1\}$ for $p=1,2,\infty$
* Some simple polyhedra and simple cones
Warning: it's easy to write down seemingly simple set $C$, and $P_C$ can turn out to be very hard! E.g., generally hard to project onto arbitrary polyhedron $C= \{x: Ax \le b\}$
Note: projected gradient descent works too, more next time

## Better approach?
Upside to the subgradient method: broad applicability. Downside: $O(1/\epsilon^2)$ convergence rate over problem class of convex, Lipschitz functions is rally slow

Nonsmooth first-order methods: iterative methods updating $x^{(0)}$ in
$$
x^{(0)} + \text{span}\{g^{0}, g^{(1)},..., g^{(k-1)}\}
$$
where the subgradient $g^{(0)},g^{(1)},..., g^{(k-1)}$ come from weak oracle

> [!Theorem] (Nesterov)
> For any $k \le n-1$ and starting point $x^{(0)}$, there is a function in the problem class such that any nonsmooth first-order method satisfies $$ f (x^{(k)}) - f^* \ge \frac{RG}{2(1+\sqrt{k+1})}$$

## Improving on the subgradient method
In words, we cannot do better than the $O(1/\epsilon^2)$ rate of subgradient method (unless we go beyond nonsmooth first-order methods)

So instead of trying to improve across the board, we will focus on minimizing composite functions of the form
$$
f(x) = g(x) + h(x)
$$ where $g$ is convex and differentiable, $h$ is convex and nonsmooth but "simple".

For a lot of problems (i.e., functions $h$), we can recover the $O(1/\epsilon)$ rate of gradient descent with a simple algorithm, having important practical consequences